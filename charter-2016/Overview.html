<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8" />

    <title>Multimodal Interaction Working Group Charter</title>

    <link rel="stylesheet" href="https://www.w3.org/2005/10/w3cdoc.css" type="text/css" media="screen" />
    <link rel="stylesheet" type="text/css" href="https://www.w3.org/Guide/pubrules-style.css" />
    <link rel="stylesheet" type="text/css" href="https://www.w3.org/2006/02/charter-style.css" />
    <style type="text/css">
      ul#navbar {
        font-size: small;
      }


      dt.spec {
        font-weight: bold;
      }

      dt.spec new {
        background: yellow;
      }

      ul.out-of-scope > li {
        font-weight: bold;
      }

      ul.out-of-scope > li > ul > li{
        font-weight: normal;
      }

      .issue {
        background: cornsilk;
        font-style: italic;
      }

      .todo {
        color: #900;
      }

      footer {
        font-size: small;
      }

    </style>
  </head>
  <body>
    <header id="header">
      <aside>
        <ul id="navbar">
          <li><a href="#scope">Scope</a></li>
          <li><a href="#deliverables">Deliverables</a></li>
          <li><a href="#coordination">Coordination</a></li>
          <li><a href="#participation">Participation</a></li>
          <li><a href="#communication">Communication</a></li>
          <li><a href="#decisions">Decision Policy</a></li>
          <i class="todo">[Keep 'Patent Policy' for a Working Group, 'Patent Disclosures' for an Interest Group]</i>
          <li><a href="#patentpolicy">Patent Policy</a></li>
          <li><a href="#patentpolicy">Patent Disclosures</a></li>
          <li><a href="#licensing">Licensing</a></li>
          <li><a href="#about">About this Charter</a></li>
        </ul>
      </aside>
      <p>
        <a href="https://www.w3.org/"><img alt="W3C" height="48" src="https://www.w3.org/Icons/w3c_home" width="72" /></a>
      </p>
    </header>


    <main>
      <h1 id="title">Multimodal Interaction Working Group Charter</h1>

      <p class="mission">The <strong>mission</strong> of the <a href="">Multimodal Interaction Working Group</a> is to develop open standards that enable the following vision:</p>
<ul>
  <li>Extending the Web to allow multiple modes of interaction: 
    <ul class="sub-bullet">
      <li>GUI, Speech, Vision, Pen, Gestures, Haptic interfaces, ...</li>
    </ul>
  </li>
  <li>Anyone, Anywhere, Any device, Any time 
    <ul class="sub-bullet">
      <li>Accessible through the user's preferred modes of interaction with
        services that adapt to the device, user and environmental
      conditions</li>
    </ul>
  </li>
</ul>


      <div class="noprint">
        <p class="join"><a href="https://www.w3.org/2004/01/pp-impl/34607join">Join the Multimodal Interaction Working Group.</a></p>
      </div>

      <section id="details">
        <table class="summary-table">
          <tr id="Duration">
            <th>
              Start date
            </th>
            <td>
              <i class="todo">[dd monthname yyyy] (date of the "Call for Participation", when the charter is approved)</i>
            </td>
          </tr>
          <tr id="Duration">
            <th>
              End date
            </th>
            <td>
              <i class="todo">29 February 2020</i>
            </td>
          </tr>

          <tr class="todo">
            <th>Charter extension</th>
            <td>See <a href="#history">Change History</a>.
            </td>
          </tr>

          <tr>
            <th>
              Chairs
            </th>
            <td>
              Debbie Dahl
            </td>
          </tr>
          <tr>
            <th>
              Team Contacts
            </th>
            <td>
              Kazuyuki Ashimura (0.1 <abbr title="Full-Time Equivalent">FTE</abbr>)</i>
            </td>
          </tr>
          <tr>
            <th>
              Meeting Schedule
            </th>
            <td>
              <strong>Teleconferences:</strong> Weekly (1 main group call and 1 task force call)<br />
              <strong>Face-to-face:</strong> we will meet during the W3C's annual Technical Plenary week; additional face-to-face meetings may be scheduled by consent of the participants, usually no more than 3 per year.
            </td>
          </tr>
        </table>

        <p class="issue"><b>Note:</b> The <a href="https://www.w3.org/2015/Process-20150901/#WGCharter">W3C Process Document (2015)</a> requires “The level of confidentiality of the group's proceedings and deliverables”; however, it does not mandate where this appears. Since all W3C Working Groups should be chartered as public, this notice has been moved from the essentials table to the <a href="#public">communication section</a>.</p>
      </section>


      <section id="scope" class="scope">
        <h2>Scope</h2>
        <p><i class="todo">Brief background of landscape, technology, and relationship to the Web, users, developers, implementers, and industry.</i></p>
        <p class="todo">Note: should be shorter?</p>

<h3 id="background">Background</h3>

<p>The primary goal of this group is to develop W3C Recommendations that enable
multimodal interaction with various devices including desktop PCs, mobile
phones and less traditional platforms such as cars and intelligent home
environments including digital TVs/connected TVs. <!--
For rapid adoption on a global scale, it should be possible to add
simple multimodal capabilities to existing markup languages in a way
that is backwards compatible with widely deployed devices, and which
builds upon widespread familiarity with existing Web technologies.
-->The standards should be scalable to enable richer capabilities for
subsequent generations of multimodal devices. </p>


<p>Users will be able to provide input via speech, handwriting, motion or
keystrokes, with output presented via displays, pre-recorded and synthetic
speech, audio, and tactile mechanisms such as mobile phone vibrators and
Braille strips. Application developers will be able to provide an effective
user interface for whichever modes the user selects. To encourage rapid
adoption, the same content can be designed for use on both old and new devices.
Multimodal access capabilities depend on the devices used.
For example, users of multimodal devices which include not only
keypads but also touch panel, microphone and motion sensor can enjoy
all the possible modalities, while users of devices with restricted
capability prefer simpler and lighter modalities like keypads and
voice.Extensibility to new modes of interaction is important,
especially considered in the context of the current proliferation of
types of sensor input, for example, in biometric identification and
medical applications.
</p>


<h4 id="motivation">Motivation</h4>
<p>
As technology evolves the ability to process modalities grows.

The office computer is an example. From punch cards, to keyboards to
mice and touch screens, the machine has seen a variety of modalities
come and go.

The mediums that enable interaction with a piece of technology
continue to expand as increasingly sophisticated hardware is
introduced into computing devices.

The medium mode relationship is one of many to many. As mediums are
introduced into a system, they may introduce new modes or leverage
existing ones.

For example: a system may have a tactile medium of touch and a visual
medium of eye tracking. These two mediums may share the same mode,
such as ink.

Devices increasingly expand the ability to weave mediums and modes
together. Modality components do not necessarily have to be devices.

Taking into consideration the increasing pervasiveness of computing,
the need to handle diverse sets of interactions introduces a need for
a scalable multimodal architecture that allows for rich human
interaction.

Systems currently leveraging the multimodal architecture include TV,
healthcare, automotive technology, heads up displays and personal
assistants. MMI enables interactive, interoperable, smart systems.
</p>


<h4 id="mmi-ecosystem">MMI Ecosystem Example</h4>
<p>
The diagram below shows an example of an ecosystem of interacting
Multimodal Architecture components. In this example, components are
located in the home, the car, a smartphone, and the Web. User
interaction with these components is mediated by devices such as
smartphones or wearables. The MMI Architecture, along with EMMA,
provides a layer that virtualizes user interaction on top of generic
Internet protocols such as WebSockets and HTTP, or on top of more
specific protocols such as ECHONET.
</p>

<a id="mmi-figure"></a>
<img class="center" width="100%" src="https://www.w3.org/2013/10/MMI.png" alt="MMI Overview" />
<p class="caption">Example of Multimodal Interaction Ecosystem</p>

<p>
As an example, a user in this ecosystem might be driving home in the
car, and want to start their rice cooker.The user could say something
like "start the rice cooker" or "I want to turn on the rice
cooker". MMI Life Cycle events flowing among the smartphone, the cloud
and the home network result in a command being sent to the rice cooker
to turn on.  To confirm that the rice cooker has turned on, voice
output would be selected as the best feedback mechanism for a user who
is driving. Similar scenarios could be constructed for interaction
with a wide variety of other devices in the home, the car, or in other
environments.
</p>

<p>
Note that precise synchronization of all the related entities is
becoming more and more important, because network connection is
getting very fast and there is a possibility of monitoring sensors
on the other side of the world or even on an artificial
satellite.
</p>


<h4 id="mmi-layer-cake">Possible Architecture of Advanced MMI-based Web Applications</h4>
<p>
The diagram below shows how the MMI-based user interface relates to
other Web technologies including the following layers using various communication mechanisms, e.g., MMI Events, Indie UI Events, Touch Events and Web APIs:</p>

<ul>
<li>Presentation</li>
<li>Application</li>
<li>Device</li>
<li>Session/Transport</li>
</ul>

<p>
Note that MMI is a generic mechanism to handle event exchange for advanced
Web applications with distributed multiple entities.
</p>

<img class="center" width="100%" src="https://www.w3.org/2013/10/MMI-layer-cake.png" alt="MMI Layer cake" />
<p class="caption">Layer Cake of MMI-based Web Applications</p>


<h3 id="target_audience">Target Audience</h3>

<p>The target audience of the
<a href="http://www.w3.org/2002/mmi/">Multimodal Interaction Working Group</a>
<!--
(<a href="http://cgi.w3.org/MemberAccess/AccessRequest">member only
link</a>)
-->
are vendors and service providers of multimodal
applications, and should include a range of organizations in different industry
sectors like:</p>


<dl>
  <dt>Mobile and hand-held devices</dt>
    <dd>As a result of increasingly capable networks, devices, and speech
      recognition technology, the number of existing multimodal applications,
      especially mobile applications, is rapidly accelerating.
    </dd>

  <dt>Home appliances, e.g., TV, and home networks</dt>
    <dd>Multimodal interfaces are expected to add value to remote control of
      home entertainment systems, as well as finding a role for other systems
      around the home. Companies involved in developing embedded systems and
      consumer electronics should be interested in W3C's work on multimodal
      interaction.</dd>

  <dt>Enterprise office applications and devices</dt>
    <dd>Multimodal has benefits for desktops, wall mounted interactive
      displays, multi-function copiers and other office equipment which offer a
      richer user experience and the chance to use additional modalities like
      speech and pens to existing modalities like keyboards and mice. W3C's
      standardization work in this area should be of interest to companies
      developing client software and application authoring technologies, and
      who wish to ensure that the resulting standards live up to their needs.
    </dd>

  <dt>Intelligent IT ready cars</dt>
    <dd>With the emergence of dashboard integrated high resolution color
      displays for navigation, communication and entertainment services, W3C's
      work on open standards for multimodal interaction should be of interest
      to companies working on developing the next generation of in-car
    systems.</dd>

  <dt>Medical applications</dt>
    <dd>Mobile healthcare professionals and practitioners of telemedicine will
      benefit from multimodal standards for interactions with remote patients
      as well as for collaboration with distant colleagues. <br />
    </dd>
</dl>

<p>Speech is a very practical way for those industry sectors to
  interact with smaller devices, allowing one-handed and hands-free
  operation. Users benefit from being able to choose which modalities
  they find convenient in any situation. General
  purpose <strong>personal assistant </strong>applications such Apple
  Siri, Google Now, Google Glass, Anboto Sherpa, XOWi, Speaktoit for
  Android, Indigo for Android and Cluzee for Android are becoming
  widespread. These applications feature interfaces that include
  speech input and output, graphical input and output, natural
  language understanding, and back-end integration with other
  software. More recently, enterprise personal assistant platforms
  such as Nuance Nina, Angel Lexee and Openstream Cue-me have also
  been introduced. In addition to multimodal interfaces, enterprise
  personal assistant applications frequently include deep integration
  with enterprise functionality. Personal Assistant functionality is
  merging with earlier Multimodal
  <strong>Voice Search</strong> applications which have been implemented in applications by a number      
  of companies, including Google, Microsoft, Yahoo, Novauris, AT&amp;T, Vocalia,      and
Sound Hound. The Working Group      should be of interest 
  to companies developing smart phones and personal      digital assistants or 
  who are interested in providing tools and      technology to support the 
  delivery of multimodal services to such      devices. </p>

<h3 id="work_to_do">Work to do</h3>

<p>To solicit industrial expertise for the expected multimodal
interaction ecosystem, the group held the following two Webinars and a
Workshop:</p>

<ul>

<li>
W3C Webinar on
"<a href="https://event.on24.com/eventRegistration/EventLobbyServlet?target=registration.jsp&amp;eventid=567980&amp;sessionid=1&amp;key=3D02EAC371B0A72EA1C51DCA6CE14996&amp;sourcepage=register">Developing
Portable Mobile Applications with Compelling User Experience using the
W3C MMI Architecture</a>" on 31 January 2014
</li>

<li>
W3C Workshop on 
"<a href="https://www.w3.org/2013/07/mmi/">Rich Multimodal Application Development</a>" on 22-23 July 2013
</li>

<li>
W3C Webinar on
"<a href="https://event.on24.com/eventRegistration/EventLobbyServlet?target=registration.jsp&amp;eventid=685284&amp;sessionid=1&amp;key=BAD08013B1C4C4E23630128792130492&amp;sourcepage=register">Discovery in Distributed Multimodal Interaction</a>"
on 24 September 2013
</li>

</ul>


<p>
Those Webinars and Workshop were aimed at Web developers who may find
it daunting to incorporate innovative input and output methods such as
speech, touch, gesture and swipe into their applications, given the
diversity of devices and programming techniques available today.

During those events, we had great discussion on rich multimodal Web
applications, and clarified many new use cases and requirements for
enhanced multimodal user-experiences, e.g., distributed/dynamic
applications depend on the ability of devices and environments to find
each other and learn what modalities they support.

Target industry areas include health care, financial services,
broadcasting, automotive, gaming and consumer devices.

Example use cases include:
</p>

<ul>
<li>Advanced TV Applications/Services:
  <ul>
  <li>Authenticated, authorized users can watch TV from any device</li>
  <li>Multiple devices in sync with each other, delivery mechanism for supplementary content</li>
  <li>Invoking personalized TV home screen using facial recognition, customized program guide, content, applications</li>
  </ul>
</li>

<li>Appliances and Cars:
  <ul>
  <li>Appliances connected to the cloud, e.g., rice cookers, fridges, microwaves and medical devices</li>
  <li>Devices connected using NFC enabled smartphones</li>
  <li>Cars connected to the cloud and road infrastructure giving traffic information, recommendations and music</li>
  </ul>
</li>

<li>Interaction with Intelligent Sensors and Devices:
  <ul>
  <li>Devices dynamically become part of a group in the workplace</li>
  <li>Sensor/camera input &mdash; integration with input from other modalities</li>
  <li>Voice enabled personal assistant</li>
  <li>Browsers take multiple/various inputs &mdash; GUI, gesture and sensor input</li>
  </ul>
</li>
</ul>
 

<p>
The main target of "Multimodal Interaction" used to be the interaction
between computer systems and human, so the group have been working on
human-machine interfaces, e.g., GUI, speech and touch.

However, after holding the above events, it was clarified that MMI
Architecture and EMMA are useful for not only human-machine interface
but also integration with machine input/output.

Therefore there is a need to consider how to integrate existing non
Web-based embedded systems and Web-based systems, and the group would
like to provide a solution to deal with the issue by tackling the
following standardization work:
</p>


<dl>
<dt><a id="mmi-architecture" name="mmi-architecture"></a>Multimodal Architecture and Interfaces (MMI Architecture)</dt>
  <dd>
  <p>
  During
  the <a href="http://www.w3.org/2011/03/mmi-charter.html">previous
  charter period</a> the group brought the <a href="http://www.w3.org/TR/2012/REC-mmi-arch-20121025/">Multimodal Architecture and Interfaces (MMI Architecture) specification</a> to Recommendation.

  The group will continue maintaining the specification.
  </p>

  <p>
  The group may expand
  the <a href="http://www.w3.org/TR/2008/WD-mmi-arch-20081016/">Multimodal
  Architecture and Interfaces</a> specification by including the
  description of Modality Components Registration &amp; Discovery
  Working Group Note so that the specification would fit the need of
  industory sectors.
  </p>
  </dd>

<dt><a id="modality-components-registration-and-discovery"
  name="modality-components-registration-and-discovery"></a>Modality
  Components Registration &amp; Discovery</dt>


    <dd>
    <p>The group has been investigating how various Modality
      Components in the MMI Architecture which are responsible for
      controlling the various input and output modalities on various
      devices can register their capabilities and existence with the
      Interaction Manager.

      The possible examples of registration information of Modality
      Components include capabilities of Modality Components such as
      Languages supported, ink capture and playback, media capture and
      playback (audio, video, images, sensor data), speech
      recognition, text to speech, SVG, geolocation and emotion. </p>

<!--
      <p>It should be noted here that any single Modality Component in the MMI
      Architecture can potentially encompass multiple physical devices, also
      more than one Modality Components could be included in a single device.
      Given that distinction, the focus of this group is to investigate and
      document the process for <strong>discovery of Modality Components and
      their capabilities rather than that for discovery of devices and
      services</strong>, which may have been addressed by other working groups,
      e.g., the <a href="http://www.w3.org/2009/dap/">Device APIs Working
      Group</a>. The Multimodal Interaction Working Group itself will not
      define any JavaScript APIs for device discovery or registration. </p>
-->

<p>
The group's proposal for the description of multimodal properties
focuses on the use of semantic Web services technologies over well
known network layers like UPnP, DLNA, Zeroconf or the HTTP protocol.
Intended to be an extension of the current MMI Architecture and
Interfaces recommendation, the goal is to enhance the discovery of
services provided by Modality Components on devices using well defined
descriptions from a multimodal perspective.

We believe that this approach will enhance the autonomous behavior of
multimodal applications, providing a robust perception of the
user-system needs and exchanges, and 
helping control the semantic integration of the human-computer
interaction from a context and functioning environment perspective.

As a result, the challenge of discovery is addressed using description
tools provided by the field of web services, like WSDL 2.0 documents
annotated with semantic data (SAWSDL) in owl or owl-s languages using
the modelReference attribute.

This extension adds functionalities for the discovering and
registration of the Modality Components by using semantic WSDL
descriptions of the contextual preconditions (QoS, price and other non
functional information), processes and results provided by these
components. These documents, following the WSDL 2.0 structure can
either be expressed in xml format or in the lighter Json or Yaml
formats.
</p>

      <p>The group generated a Working Group Note on Registration
      &amp; Discovery, and based on that document the group will
      generate a document which describes the detailed definition of
      the format and process of Registration of Modality Components as
      well as the process for the Interaction Manager to discover the
      existence and current capabilities of Modality Components. This
      document is expected to become a recommendation track
      document.</p>
</dd>

<dt>Multimodal Authoring</dt>
<dd>
<p>
The group will investigate and recommend how various W3C 
languages can be extended for use in a multimodal environment using the 
multimodal
<a href="http://www.w3.org/TR/2008/WD-mmi-arch-20081016/#ComponentInterface">Life-Cycle Events</a>.

The group may prepare W3C Notes on how the following languages can
participate in multimodal specifications by incorporating the life
cycle events from the multimodal architecture: XHTML, VoiceXML,
MathML, SMIL, SVG, InkML and other languages that can be used in a
multimodal environment.

The working group is also interested in investigating how CSS and
<a href="http://www.w3.org/TR/DPF/">
Delivery Context: Client Interfaces (DCCI)</a>
can be used to support Multimodal Interaction applications, and if
appropriate, may write a W3C Note. 
</p>
</dd>

  <dt>Extensible Multi-Modal Annotations (EMMA) 2.0</dt>

    <dd>
      <p>
      EMMA is a data exchange format for the interface between different
      levels of input processing and interaction management in multimodal and
      voice-enabled systems.

      Whereas the MMI Architecture defines communication among
      components of multimodal systems, including distributed and
      dynamic systems, EMMA provides information used in
      communications with users.

      For example, EMMA provides the means for input processing
      components, such as speech recognizers, to annotate application specific
      data with information such as confidence scores, time stamps, and input
      mode classification (e.g. key strokes, touch, speech, or pen).

      It also provides mechanisms for representing alternative
      recognition hypotheses including lattice and groups and
      sequences of inputs.

      EMMA supersedes earlier work on the
      natural language semantics markup language (NLSML) in the Voice
      Browser Activity.
      </p>

      <p>
      During the previous charter periods, the
      group brought
      <a href="http://www.w3.org/TR/2009/REC-emma-20090210/">EMMA
      Version 1.0</a> to Recommendation.  Also the group generated
      Working Drafts of
      the <a href="http://www.w3.org/TR/emma11/">EMMA Version 1.1
      specification</a> based on the feedback from the MMI Webinars
      and the MMI workshop in 2013.
      </p>

      <p>
      In the period defined by this charter, the group will publish a
      new EMMA 2.0 version of the specification which incorporates new
      features that address issues brought up during the development
      of EMMA 1.1.

      The group won't do any more work on EMMA Version 1.1, but the
      work done on EMMA 1.1 will be folded into EMMA Version 2.0.
      </p>

      <p>
      Note that the original EMMA 1.0 specification focused on
      representing multimodal user inputs, and EMMA 2.0 will add
      representation of multimodal outputs directed to users.
      </p>

      <p>New features for EMMA 2.0 includes but not limited to the following:</p>
      <ul>
        <li>Broader range of semantic representation
        formats, e.g., JSON and RDF</li>
        <li>Handling machine output</li>
        <li>Incremental recognition results, e.g., speech and gesture</li>
      </ul>
    </dd>


<dt>Semantic Interpretation for Multimodal Interaction</dt>
<dd>
<p>
During the MMI Webinars and the MMI workshop, there was discussion on
possible mechanism to handle semantic interpretation for multimodal
interaction, e.g., gesture recognition, like the one for speech
recognition, i.e., Semantic Interpretation for Speech Recognition (SISR).
</p>

<p>
The group will start to see how to apply SISR to semantic
interpretation for non-speech Modality Components, and discuss
concrete use cases and requirements.

However, the group will not generate a new version of SISR
specification.
</p>
</dd>


<dt>Maintenance work</dt>
    <dd>
    <p>The group will be maintaining the following specifications:</p>
    <ul>
    <li><a href="http://www.w3.org/TR/2009/REC-emma-20090210/">EMMA 1.0</a></li>
    <li><a href="http://www.w3.org/TR/2011/REC-InkML-20110920/">InkML</a></li>
    <li><a href="http://www.w3.org/TR/2012/REC-mmi-arch-20121025/">MMI Architecture</a></li>
    <li><a href="http://www.w3.org/TR/2014/REC-emotionml-20140522/">EmotionML</a></li>
    </ul>

    <p>The group will also consider publishing additional versions of
      the specifications depending on feedback from the user commuinty
      and industry sectors.
    </p>
    </dd>
</dl>

        <div id="section-out-of-scope">
          <h3 id="out-of-scope">Out of Scope</h3>
            <p>The following features are out of scope, and will not be addressed by this <i class="todo">(Working|Interest)</i> group.</p>

            <ul class="out-of-scope">
            </ul>
        </div>

        <div>
          <h3>Success Criteria</h3>
          <p>In order to advance to <a href="https://www.w3.org/2015/Process-20150901/#RecsPR" title="Proposed Recommendation">Proposed Recommendation</a>, each specification is expected to have <a href="https://www.w3.org/2015/Process-20150901/#implementation-experience">at least two independent implementations</a> of each of feature defined in the specification.</p>
          <p>Each specification should contain a section detailing any known security or privacy implications for implementers, Web authors, and end users.</p>
          <p><i class="todo">For specifications of technologies that directly impact user experience, such as content technologies, as well as protocols and APIs which impact content: </i>Each specification should contain a section describing known impacts on accessibility to users with disabilities, ways the specification features address them, and recommendations for minimizing accessibility problems in implementation.</p>

<p class="todo">For each document to advance to proposed Recommendation, the group will
produce a technical report with at least two independent and interoperable
implementations for each required feature. The working group anticipates two
interoperable implementations for each optional feature but may reduce the
criteria for specific optional features. </p>

        </div>
      </section>



      <section id="deliverables">
        <h2>
          Deliverables
        </h2>
        <p class="issue"><b>Issue:</b> I've removed the milestones table, which is always wildly inaccurate, and included the milestones as projected completion dates in the description template for each deliverable. Up-to-date milestones tables should be maintained on the group page and linked to from this charter.</p>

        <p>More detailed milestones and updated publication schedules are available on the <a href="https://www.w3.org/wiki/[groupname]/PubStatus">group publication status page</a>.</p>

        <p><i>Draft state</i> indicates the state of the deliverable at the time of the charter approval. <i>Expected completion</i> indicates when the deliverable is projected to become a Recommendation, or otherwise reach a stable state.</p>

        <div id="normative">
          <h3>
            Normative Specifications
          </h3>
          <p>
            The <i class="todo">(Working|Interest)</i> Group will deliver the following W3C normative specifications:
          </p>
          <dl>
            <dt id="web-foo" class="spec"><a href="#">Web <i class="todo">[spec name]</i></a></dt>
            <dd>
              <p>This specification defines <i class="todo">[concrete description]</i>.</p>

              <p class="draft-status"><b>Draft state:</b> <i class="todo">[No draft | <a href="#">Use Cases and Requirements</a> | <a href="#">Editor's Draft</a> | <a href="#">Member Submission</a> | <a href="#">Adopted from WG/CG Foo</a> | <a href="#">Working Draft</a>]</i></p>

              <p class="milestone"><b>Expected completion:</b> <i class="todo">[Q1–4 yyyy]</i></p>
            </dd>
          </dl>

<p>The following documents are expected to become W3C Recommendations:</p>
<ul>
  <li>EMMA: Extensible MultiModal Annotation markup language Version 2.0
  (based on the current 1.1 Working Draft)</li>
<li>Modality Components Registration &amp; Discovery</li>
</ul>

<p>The following documents are Working Group Notes and are not expected
to advance toward Recommendation:</p>
<ul>
<li>Authoring Examples</li>
<li>Multimodality and Accessibility</li>
<li>Semantic Interpretation for Multimodality</li>
</ul>

        </div>

        <div id="ig-other-deliverables">
          <h3>
            Other Deliverables
          </h3>
          <p>
            Other non-normative documents may be created such as:
          </p>
          <ul>
            <li>Use case and requirement documents;</li>
            <li>Test suite and implementation report for the specification;</li>
            <li>Primer or Best Practice documents to support web developers when designing applications.</li>
          </ul>
        </div>

        <div id="timeline">
          <h3>Timeline</h3>
            <p class="todo">Put here a timeline view of all deliverables.</p>
            <ul class="todo">
              <li>Month YYYY: First teleconference</li>
              <li>Month YYYY: First face-to-face meeting</li>
              <li>Month YYYY: Requirements and Use Cases for FooML</li>
              <li>Month YYYY: FPWD for FooML</li>
              <li>Month YYYY: Requirements and Use Cases for BarML</li>
              <li>Month YYYY: FPWD FooML Primer</li>
            </ul>
        </div>
      </section>



      <section id="coordination">
        <h2>Coordination</h2>
        <p>For all specifications, this <i class="todo">(Working|Interest)</i> Group will seek <a href="https://www.w3.org/Guide/Charter.html#horizontal-review">horizontal review</a> for accessibility, internationalization, performance, privacy, and security with the relevant Working and Interest Groups, and with the <a href="https://www.w3.org/2001/tag/" title="Technical Architecture Group">TAG</a>. Invitation for review must be issued during each major standards-track document transition, including <a href="https://www.w3.org/2015/Process-20150901/#RecsWD" title="First Public Working Draft">FPWD</a> and <a href="https://www.w3.org/2015/Process-20150901/#RecsCR" title="Candidate Recommendation">CR</a>, and should be issued when major changes occur in a specification.</p>
        <p class="issue"><b>Issue:</b> The paragraph above replaces line-item liaisons and dependencies with individual horizontal groups. There's been a suggestion to name and individually link the specific horizontal groups inline, instead of pointing to a page that lists them.</p>
        <p class="issue"><b>Issue:</b> The requirement for an invitation to review after FPWD and before CR may seem a bit overly restrictive, but it only requires an invitation, not a review, a commitment to review, or even a response from the horizontal group. This compromise offers early notification without introducing a bottleneck.</p>

        <p>Additional technical coordination with the following Groups will be made, per the <a href="https://www.w3.org/2015/Process-20150901/#WGCharter">W3C Process Document</a>:</p>

      	<p class="todo">In addition to the above catch-all reference to horizontal review which includes accessibility review, please check with chairs and staff contacts of the <a href="https://www.w3.org/WAI/APA/">Accessible Platform Architectures Working Group</a> to determine if an additional liaison statement with more specific information about concrete review issues is needed in the list below.</p>

        <div>
          <h3 id="w3c-coordination">W3C Groups</h3>
          <dl>
            <dt><a href=""><i class="todo">[other name]</i> Working Group</a></dt>
            <dd><i class="todo">[specific nature of liaison]</i></dd>
          </dl>

<p>These are W3C activities that may be asked to review documents produced by
the Multimodal Interaction Working Group, or which may be involved in closer
collaboration as appropriate to achieving the goals of the Charter.</p>

<dl>
  <dt><a href="https://www.w3.org/2011/audio/">Audio WG</a></dt>
    <dd>Advanced sound and music capabilities by client-side script APIs</dd>

  <dt><a href="http://www.w3.org/community/autowebplatform/">Automotive and Web Platform BG</a></dt>
    <dd>Web APIs for vehicle data</dd>

  <dt><a href="http://www.w3.org/Style/">CSS WG</a></dt>
    <dd>styling for multimodal applications</dd>

  <dt><a href="http://www.w3.org/2009/dap/">Device APIs WG</a></dt>
    <dd>client-side APIs for developing Web Applications and Web Widgets that
      interact with devices services</dd>

  <dt><a href="http://www.w3.org/MarkUp/Forms/">Forms</a></dt>
    <dd>separating forms into data models, logic and presentation</dd>

  <dt><a href="http://www.w3.org/2008/geolocation/">Geolocation WG</a></dt>
    <dd>handling geolocation information of devices</dd>

  <dt><a href="http://www.w3.org/html/wg/">HTML WG</a></dt>
    <dd>HTML5 browser as Graphical User Interface</dd>

  <dt><a href="http://www.w3.org/International/core/">Internationalization (I18N) WG</a></dt>
    <dd>support for human languages across the world</dd>

  <dt><a href="http://www.w3.org/WAI/IndieUI/">Independent User Interface (Indie UI) WG</a></dt>
    <dd>Event models for APIs that facilitate interaction in Web
    applications that are input method independent and accessible to
    people with disabilities</dd>

  <dt><a href="http://www.w3.org/Math/">Math WG</a></dt>
    <dd>InkML includes subset of MathML functionalities with the
      &lt;mapping&gt; element </dd>

  <dt><a href="http://www.w3.org/Graphics/SVG/">SVG WG</a></dt>
    <dd>graphical user interfaces</dd>

  <dt><a href="http://www.w3.org/Privacy/">Privacy IG</a></dt>
    <dd>support of privacy in Web standards</dd>

  <dt><a href="http://www.w3.org/blog/hcls/">Semantic Web Health Care and Life Sciences IG</a></dt>
  <dd>Semantic Web technologies across health care, life sciences, clinical
research and translational medicine</dd>

  <dt><a href="https://www.w3.org/2001/sw/interest/">Semantic Web IG</a></dt>
    <dd>the role of metadata</dd>

  <dt><a href="http://www.w3.org/community/speech-api/">Speech API CG</a></dt>
    <dd>integrating speech technology in HTML5</dd>

  <dt><a href="http://www.w3.org/2012/sysapps/">System Applications WG</a></dt>
    <dd>Runtime environment, security model and associated APIs for
    building Web applications with comparable capabilities to native
    applications</dd>

  <dt><a href="http://www.w3.org/AudioVideo/TT/">Timed Text
  WG</a></dt>
    <dd>synchronized text and video</dd>

  <dt><a href="http://www.w3.org/Voice/">Voice Browsers WG</a></dt>
    <dd>voice interfaces</dd>

  <dt><a href="http://www.w3.org/WAI/PF/">WAI Protocols and Formats WG</a></dt>
    <dd>ensuring accessibility for Multimodal systems</dd>

  <dt><a href="http://www.w3.org/WAI/UA/">WAI User Agent Accessibility
  Guidelines WG</a></dt>
    <dd>accessibility of user interface to Multimodal systems</dd>

  <dt><a href="http://www.w3.org/2011/webtv/">Web and TV IG</a></dt>
    <dd>TV and other CE devices as Modality Components for Multimodal
    systems</dd>

  <dt><a href="https://www.w3.org/Mobile/IG/">Web and Mobile IG</a></dt>
    <dd>browsing the Web from mobile devices</dd>

  <dt><a href="https://www.w3.org/annotation/">Web Annotations WG</a></dt>
    <dd>Generic data model for annotations and the basic
    infrastructural elements to make it deployable in browsers and
    reading systems</dd>

  <dt><a href="http://www.w3.org/2008/webapps/">Web Applications WG</a></dt>
    <dd>Specifications for webapps, including standard APIs for client-side
      development, Document Object Model (DOM) and a packaging format for installable webapps</dd>

  <dt><a href="http://www.w3.org/community/wot/">Web of Things CG</a></dt>
    <dd>Adoption of Web technologies as a basis for enabling services
    for the combination of the Internet of Things</dd>

  <dt><a href="https://www.w3.org/2011/04/webrtc/">WebRTC WG</a></dt>
    <dd>Client-side APIs to enable Real-Time Communications in Web browsers</dd>

  <dt><a href="http://www.w3.org/Security/wiki/IG">Web Security IG</a></dt>
    <dd>improving standards and implementations to advance the
    security of the Web</dd>

</dl>

<p>Furthermore, Multimodal Interaction Working Group expects to follow these
W3C Recommendations:</p>
<ul>
  <li><a href="http://www.w3.org/TR/qaframe-spec/" shape="rect">QA Framework:
    Specification Guidelines</a>.</li>
  <li><a href="http://www.w3.org/TR/charmod/" shape="rect">Character Model for
    the World Wide Web 1.0: Fundamentals</a></li>
  <li><a href="http://www.w3.org/TR/webarch/" shape="rect">Architecture of the
    World Wide Web, Volume I</a></li>
</ul>
          <p class="issue"><b>Note:</b> Do not list horizontal groups here, only specific WGs relevant to your work.</p>

          <h3 id="external-coordination">External Organizations</h3>
          <dl>
            <dt><a href=""><i class="todo">[other name]</i> Working Group</a></dt>
            <dd><i class="todo">[specific nature of liaison]</i></dd>
          </dl>
<p>This is an indication of external groups with complementary goals to the
Multimodal Interaction activity. W3C has formal liaison agreements with some of
them, e.g. VoiceXML Forum.

The group will coordinate with other new related organizations as well if needed.
</p>

<dl>
  <dt><a href="http://www.3gpp.org">3GPP</a></dt>
    <dd>protocols and codecs relevant to multimodal applications</dd>

  <dt><a href="http://www.dlna.org/home">DLNA</a></dt>
    <dd>open standards and widely available industry specifications for
      entertainment devices and home network</dd>

  <dt><a href="http://www.echonet.gr.jp/english/">Echonet Consortium</a></dt>
    <dd>standard communication protocol and data format for smart home</dd>

  <dt><a href="http://www.etsi.org">ETSI</a></dt>
    <dd>work on human factors and command vocabularies</dd>

  <dt><a href="http://www.ietf.org">IETF</a></dt>
    <dd>SpeechSC working group has a dependency on EMMA for the MRCP v2
      specification.</dd>

  <dt><a href="http://www.itu.int/en/ITU-T/studygroups/2013-2016/13/Pages/default.aspx">ITU-T (SG13)</a></dt>
    <dd>generic architecture for device integration</dd>

  <dt><a href="http://www.jtc1.org/">ISO/IEC JTC 1</a>/SC 37 Biometrics</dt>
    <dd>user authentication in multimodal applications</dd>

  <dt><a
  href="http://www.oasis-open.org/committees/tc_home.php?wg_abbrev=bias"><acronym
  title="Organization for the Advancement of Structured Information Standards">OASIS
  </acronym> <acronym title="Biometric Identity Assurance Services">BIAS
  </acronym> Integration <acronym title="Technical Committee">TC </acronym>
  </a> </dt>
    <dd>defining methods for using biometric identity assurance in
      transactional Web services and SOAs <br />
      having initial discussion on how EMMA could be used as a data format for
      biometrics </dd>

  <dt><a href="http://www.upnp.org/">UPnP Forum</a></dt>
    <dd>Networking protocols permits networked devices, such as
    personal computers, printers, Internet gateways, Wi-Fi access
    points and mobile devices to seamlessly discover each other's
    presence on the network and establish functional network services</dd>

  <dt><a href="http://www.voicexml.org/">VoiceXML Forum</a></dt>
    <dd>an industry association for VoiceXML</dd>
</dl>

        </div>
      </section>



      <section class="participation">
        <h2 id="participation">
          Participation
        </h2>
        <p>
          To be successful, this <i class="todo">(Working|Interest)</i> Group is expected to have 6 or more active participants for its duration, including representatives from the key implementors of this specification, and active Editors and Test Leads for each specification. The Chairs, specification Editors, and Test Leads are expected to contribute half of a day per week towards the (Working|Interest) Group. There is no minimum requirement for other Participants.
        </p>
        <p>
          The group encourages questions, comments and issues on its public mailing lists and document repositories, as described in <a href='#communication'>Communication</a>.
        </p>
        <p>
          The group also welcomes non-Members to contribute technical submissions for consideration upon their agreement to the terms of the <a href="https://www.w3.org/Consortium/Patent-Policy/">W3C Patent Policy</a>.
        </p>
      </section>



      <section id="communication">
        <h2>
          Communication
        </h2>
        <p id="public">
          Technical discussions for this <i class="todo">(Working|Interest)</i> Group are conducted in <a href="https://www.w3.org/2015/Process-20150901/#confidentiality-levels">public</a>: the meeting minutes from teleconference and face-to-face meetings will be archived for public review, and technical discussions and issue tracking will be conducted in a manner that can be both read and written to by the general public. Working Drafts and Editor's Drafts of specifications will be developed on a public repository, and may permit direct public contribution requests.
        The meetings themselves are not open to public participation, however.
        </p>
        <p>
          Information about the group (including details about deliverables, issues, actions, status, participants, and meetings) will be available from the <a href=""><i class="todo">[name]</i> (Working|Interest) Group home page.</a>
        </p>
        <p>
          Most <i class="todo">[name]</i> (Working|Interest) Group teleconferences will focus on discussion of particular specifications, and will be conducted on an as-needed basis.
        </p>
        <p>
          This group primarily conducts its technical work  <i class="todo">pick one, or both, as appropriate:</i> on the public mailing list <a class="todo" id="public-name" href="mailto:public-[email-list]@w3.org">public-<i class="todo">[email-list]</i>@w3.org</a> (<a class="todo" href="http://lists.w3.org/Archives/Public/public-[email-list]/">archive</a>)
          <i class="todo">or</i> on <a class="todo" id="public-github" href="[link to Github repo]">GitHub issues</a>.
          The public is invited to review, discuss and contribute to this work.
        </p>
        <p>
          The group may use a Member-confidential mailing list for administrative purposes and, at the discretion of the Chairs and members of the group, for member-only discussions in special cases when a participant requests such a discussion.
        </p>
      </section>



      <section id="decisions">
        <h2>
          Decision Policy
        </h2>
        <p>
          This group will seek to make decisions through consensus and due process, per the <a href="https://www.w3.org/2015/Process-20150901/#Consensus"> W3C Process Document (section 3.3</a>). Typically, an editor or other participant makes an initial proposal, which is then refined in discussion with members of the group and other reviewers, and consensus emerges with little formal voting being required.</p>
        <p>
           However, if a decision is necessary for timely progress, but consensus is not achieved after careful consideration of the range of views presented, the Chairs may call for a group vote, and record a decision along with any objections.
        </p>
        <p>
          To afford asynchronous decisions and organizational deliberation, any resolution (including publication decisions) taken in a face-to-face meeting or teleconference will be considered provisional.

          A call for consensus (CfC) will be issued for all resolutions (for example, via email and/or web-based survey), with a response period from one week to 10 working days, depending on the chair's evaluation of the group consensus on the issue.

          If no objections are raised on the mailing list by the end of the response period, the resolution will be considered to have consensus as a resolution of the <i class="todo">(Working|Interest)</i> Group.
        </p>
        <p>
          All decisions made by the group should be considered resolved unless and until new information becomes available, or unless reopened at the discretion of the Chairs or the Director.
        </p>
        <p>
          This charter is written in accordance with the <a href="https://www.w3.org/Consortium/Process/policies#Votes">W3C Process Document (Section 3.4, Votes)</a>, and includes no voting procedures beyond what the Process Document requires.
        </p>
      </section>



      <section id="patentpolicy">
      <p><i class="todo">[Keep 'Patent Policy' for a Working Group, 'Patent Disclosures' for an Interest Group]</i></p>

        <h2>
          Patent Policy
        </h2>
        <p>
          This Working Group operates under the <a href="http://w3.org/Consortium/Patent-Policy-20040205/">W3C Patent Policy</a> (5 February 2004 Version). To promote the widest adoption of Web standards, W3C seeks to issue Recommendations that can be implemented, according to this policy, on a Royalty-Free basis.

          For more information about disclosure obligations for this group, please see the <a href="https://www.w3.org/2004/01/pp-impl/">W3C Patent Policy Implementation</a>.
        </p>

        <h2>Patent Disclosures </h2>
        <p>The Interest Group provides an opportunity to
          share perspectives on the topic addressed by this charter. W3C reminds
          Interest Group participants of their obligation to comply with patent
          disclosure obligations as set out in <a shape="rect" href="https://www.w3.org/Consortium/Patent-Policy/#sec-Disclosure">Section
            6</a> of the W3C Patent Policy. While the Interest Group does not
          produce Recommendation-track documents, when Interest Group
          participants review Recommendation-track specifications from Working
          Groups, the patent disclosure obligations do apply. For more information about disclosure obligations for this group,
          please see the <a href="https://www.w3.org/2004/01/pp-impl/">W3C
            Patent Policy Implementation</a>.</p>
      </section>



      <section id="licensing">
        <h2>Licensing</h2>
        <p>This <i class="todo">(Working|Interest)</i> Group will use the <i class="todo">[pick a license, one of:]</i>  <a href="https://www.w3.org/Consortium/Legal/copyright-documents">W3C Document license</a> | <a href="https://www.w3.org/Consortium/Legal/copyright-software">W3C Software and Document license</a> for all its deliverables.</p>
      </section>



      <section id="about">
        <h2>
          About this Charter
        </h2>
        <p>
          This charter has been created according to <a href="https://www.w3.org/Consortium/Process/groups#GAGeneral">section 5.2</a> of the <a href="https://www.w3.org/Consortium/Process">Process Document</a>. In the event of a conflict between this document or the provisions of any charter and the W3C Process, the W3C Process shall take precedence.
        </p>

        <section id="history">
          <h3>
            Charter History
          </h3>
          <p class="issue"><b>Note:</b>Display this table and update it when appropriate. Requirements for charter extension history are documented in the <a href="https://www.w3.org/Guide/Charter#extension">Charter Guidebook (section 4)</a>.</p>

          <p>The following table lists details of all changes from the initial charter, per the <a href="https://www.w3.org/2015/Process-20150901/#CharterReview">W3C Process Document (section 5.2.3)</a>:</p>

          <table class="history">
            <tbody>
              <tr>
                <th>
                  Charter Period
                </th>
                <th>
                  Start Date
                </th>
                <th>
                  End Date
                </th>
                <th>
                  Changes
                </th>
              </tr>
              <tr>
                <th>
                  <a class="todo" href="">Initial Charter</a>
                </th>
                <td>
                  <i class="todo">[dd monthname yyyy]</i>
                </td>
                <td>
                  <i class="todo">[dd monthname yyyy]</i>
                </td>
                <td>
                  <i class="todo">none</i>
                </td>
              </tr>
              <tr>
                <th>
                  <a class="todo" href="">Charter Extension</a>
                </th>
                <td>
                  <i class="todo">[dd monthname yyyy]</i>
                </td>
                <td>
                  <i class="todo">[dd monthname yyyy]</i>
                </td>
                <td>
                  <i class="todo">none</i>
                </td>
              </tr>
              <tr>
                <th>
                  <a class="todo" href="">Rechartered</a>
                </th>
                <td>
                  <i class="todo">[dd monthname yyyy]</i>
                </td>
                <td>
                  <i class="todo">[dd monthname yyyy]</i>
                </td>
                <td>
                  <p class="todo">[description of change to charter, with link to new deliverable item in charter] <b>Note:</b> use the class <code>new</code> for all new deliverables, for ease of recognition.</p>
                </td>
              </tr>
            </tbody>
          </table>
        </section>

<p>This charter for the Multimodal Interaction Working Group has been created
according to <a href="/Consortium/Process/groups#GAGeneral"
shape="rect">section 6.2</a> of the <a href="/Consortium/Process"
shape="rect">Process Document</a>. In the event of a conflict between this
document or the provisions of any charter and the W3C Process, the W3C Process
shall take precedence.</p>

<p>The most important changes from the
<a href="http://www.w3.org/2011/03/mmi-charter.html">previous charter</a>
are:</p>

<dl>
<!--
<li>reduced Team staff resource (FTE %) from 30 to 20</li>
-->

<dt>1.1 Background</dt>
<dd>
  <ul>
  <li>added description on the background with the motivation as well
  as an example of Multimodal Interaction ecosystem and a possible
  architecture of advanced MMI-based Web applications.</li>
  </ul>
</dd>

<dt>1.2 Target Audience</dt>
<dd>
  <ul>
  <li>updated the description to better fit the recent situation.</li>
  </ul>
</dd>

<dt>1.3 Work to do</dt>
<dd>
  <ul>
  <li>added description on new requirements for the MMI Architecture and
  related specifications from the participants in the MMI Webinars and
  the MMI Workshop held in 2013.</li>
  
  <li>updated the description on expected deliverables based on the
  feedback from the Webinars and the workshop</li>

  <li>removed "Device Handling Capability", and then added "Multimodal
  Authoring" and "Semantic Interpretation for Multimodal Interaction"
  to the deliverables based on the feedback from the Webinars and the
  workshop.</li>


  <li>added "EMMA 2.0" to the deliverables based on the work of "EMMA
  1.1".</li>

  <li>removed "EmotionML" from the deliverables because it has become
  a W3C Recommendation.</li>
</ul>
</dd>

<dt>2. Deliverables</dt>
<dd>
  <ul>
  <li>changed "Registration &amp; Discovery" from WG
  Note to a Recommendation Track document based on the feedback from
  the Webinars and the workshop</li>
  </ul>
</dd>

<dt>3. Dependencies</dt>
<dd>
  <ul>
  <li>updated the lists of related internal/external groups</li>
  </ul>
</dd>
</dl>

      </section>
    </main>

    <hr />

    <footer>
      <address>
        <i class="todo"><a href="mailto:">[team contact name]</a></i>
      </address>

      <p class="copyright">
        <a href="https://www.w3.org/Consortium/Legal/ipr-notice#Copyright">Copyright</a> ©
        <i class="todo">[yyyy]</i>
        <a href="https://www.w3.org/"><abbr title="World Wide Web Consortium">W3C</abbr></a><sup>®</sup>
        (
        <a href="https://www.csail.mit.edu/"><abbr title="Massachusetts Institute of Technology">MIT</abbr></a>,
        <a href="https://www.ercim.eu/"><abbr title="European Research Consortium for Informatics and Mathematics">ERCIM</abbr></a>,
        <a href="https://www.keio.ac.jp/">Keio</a>,
        <a href="http://ev.buaa.edu.cn/">Beihang</a>
        ), All Rights Reserved.

        <abbr title="World Wide Web Consortium">W3C</abbr> <a href="https://www.w3.org/Consortium/Legal/ipr-notice#Legal_Disclaimer">liability</a>, <a href="https://www.w3.org/Consortium/Legal/ipr-notice#W3C_Trademarks">trademark</a> and <a href="https://www.w3.org/Consortium/Legal/copyright-documents">document use</a> rules apply.
      </p>
      <p>
        <!-- $Date: 2015/04/30 16:53:49 $ -->
      </p>
    </footer>

  </body>
</html>
