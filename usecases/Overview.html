<!DOCTYPE html>
<html lang=en>
 <head>
  <title>MMI Use Cases</title>
  <meta content="text/html;charset=utf-8" http-equiv="Content-Type" />
  <meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport" />

  <style type="text/css">
   dt, dfn { font-weight: bold; font-style: normal; }
   img.extra { float: right; }
   body ins, body del { display: block; }
   body * ins, body * del { display: inline; }
   pre, code { color: black; background: transparent; font-size: inherit; font-family: monospace; }
   pre strong { color: black; font: inherit; font-weight: bold; background: yellow; }
   pre em { font-weight: bolder; font-style: normal; }
   pre.idl :link, pre.idl :visited { color: inherit; background: transparent; }
   pre.idl { border: solid thin; background: #EEEEEE; color: black; padding: 0.5em; }
   table { border-collapse: collapse; border-style: hidden hidden none hidden; }
   table thead { border-bottom: solid; }
   table tbody th:first-child { border-left: solid; }
   table td, table th { border-left: solid; border-right: solid; border-bottom: solid thin; vertical-align: top; padding: 0.2em; }
   ul.toc dfn, h1 dfn, h2 dfn, h3 dfn, h4 dfn, h5 dfn, h6 dfn { font: inherit; }
   ul.toc li ul { margin-bottom: 0.75em; }
   ul.toc li ul li ul { margin-bottom: 0.25em; }
   var sub { vertical-align: bottom; font-size: smaller; position: relative; top: 0.1em; }
   @media screen { code { color: rgb(255, 69, 0); background: transparent; } }
   code :link, code :visited { color: inherit; background: transparent; }
   .example { display: block; color: #222222; background: #FCFCFC; border-left: double; margin-left: 1em; padding-left: 1em; }
   .issue, .big-issue { color: #E50000; background: white; border: solid red; padding: 0.5em; margin: 1em 0; }
   .issue > :first-child, .big-issue > :first-child { margin-top: 0; }
   p .big-issue { line-height: 3em; }
   .note { color: green; background: transparent; }
   .note { font-family: sans-serif; }
   p.note:before { content: 'Note: '; }
   .warning { color: red; background: transparent; }
   .warning:before { font-style: normal; }
   p.warning:before { content: '\26A0 Warning! '; }
   .note, .warning { font-weight: bolder; font-style: italic; padding: 0.5em 2em; }
   .copyright { margin: 0.25em 0; }
   img { max-width: 100%; }
   h4 + .element { margin-top: -2.5em; padding-top: 2em; }
   h4 + p + .element { margin-top: -5em; padding-top: 4em; }
   .element { background: #EEEEFF; color: black; margin: 0 0 1em -1em; padding: 0 1em 0.25em 0.75em; border-left: solid #9999FF 0.25em; }
   table.matrix, table.matrix td { border: none; text-align: right; }
   table.matrix { margin-left: 2em; }
   ol#rotations li div { display: inline-block; width: 350px; margin-right: 20px; font-size: small; }
   ol#rotations li div img { width: 350px; }
   object.equation { display: block; margin: 20px 20px; width: 100%; height: inherit; }

  div.usecase {
    display: block;
    color: #222222;
    background: #FCFCFC;
    border-left-style:
    solid;border-color:#c0c0c0;
    border-left-width: 0.25em;
    margin-left: 1em;
    padding-left: 1em;
    padding-bottom: 0.5em;
  }
  </style>
  <link href="https://www.w3.org/StyleSheets/TR/W3C-ED.css" rel=stylesheet type="text/css">
 </head>

<body>
<div class="head">
<!--begin-logo-->
<p><a href="https://www.w3.org/"><img height="48" width="72" alt="W3C" src="https://www.w3.org/StyleSheets/TR/2016/logos/W3C" /></a></p>
<!--end-logo-->

<h1 id="title_heading">MMI Use Cases</h1>

<h2 class="no-num no-toc" id="w3c-editors-draft"><abbr title="World Wide Web Consortium">W3C</abbr> Editors Draft 5 December 2016</h2>

<dl>
<dt>This Version:</dt>
<dd>tbd</dd>

<dt>Latest Published Version:</dt>
<dd>tbd</dd>

<dt>Latest Editor's Draft:</dt>
<dd><a href="https://w3c.github.io/mmi/usecases/Overview.html">https://w3c.github.io/mmi/usecases/Overview.html</a></dd>

<dt>Previous version:</dt>
<dd>none</a></dd>

<dt>Editors:</dt>
<dd>Debbie Dahl, W3C Invited Expert</dd>
<dd>B Helena Rodriguez, W3C Invited Expert</dd>
<dd>Kaz Ashimura, W3C</dd>

<dt>Participate:</dt>
<dd><a href="https://github.com/w3c/mmi">We are on GitHub</a></dd>
<dd><a href="https://github.com/w3c/mmi/issues">File a bug</a></dd>
<dd><a href="https://github.com/w3c/mmi/commits">Commit history</a></dd>
<dd><a href="https://lists.w3.org/Archives/Public/www-multimodal/">Mailing list</a></dd>
</dl>

<hr/>
</div>

<h2 class="no-num no-toc" id="abstract">Abstract</h2>

<p>TBD</p>

<p>MMI as the possible user interface layer for the Web of Things (WoT) framework.
MMI integrates various user interface modalities (and devices which provide modlities) for WoT purposes.
</p>

<h2 class="no-num no-toc" id="status">Status of This Document</h2>

<p>TBD</p>


<nav id="toc">
<h2 class="no-num no-toc" id="contents">Table of Contents</h2>
<ol class="toc">
<li><a href="#Template_for_Use_Cases"><span class="tocnumber">1</span> <span class="toctext">Template for Use Cases</span></a></li>
<li><a href="#Use_Cases"><span class="tocnumber">2</span> <span class="toctext">Use Cases</span></a>
  <ol class="toc">
  <li><a href="#High_priority_use_cases"><span class="tocnumber">2.1</span> <span class="toctext">High priority use cases</span></a>
    <ol class="toc">
    <li><a href="#UC-2:_Interaction_with_Robots"><span class="tocnumber">2.1.1</span> <span class="toctext">UC-2: Interaction with Robots</span></a></li>
    <li><a href="#UC-7:_Multimodal_Appliances"><span class="tocnumber">2.1.2</span> <span class="toctext">UC-7: Multimodal Appliances</span></a></li>
    <li><a href="#UC-12:_Smart_Car_Platform"><span class="tocnumber">2.1.3</span> <span class="toctext">UC-12: Smart Car Platform</span></a></li>
    <li><a href="#UC-19_Handling_millions_of_components"><span class="tocnumber">2.1.4</span> <span class="toctext">UC-19 Handling millions of components</span></a></li>
    <li><a href="#UC-22_Smart_Hotel_Room"><span class="tocnumber">2.1.5</span> <span class="toctext">UC-22 Smart Hotel Room</span></a></li>
    <li><a href="#UC-23_Collaborative_content_sharing_in_a_smart_space"><span class="tocnumber">2.1.6</span> <span class="toctext">UC-23 Collaborative content sharing in a smart space</span></a></li>
    </ol>
  </li>

  <li><a href="#Other_use_cases"><span class="tocnumber">2.2</span> <span class="toctext">Other use cases</span></a>
    <ol class="toc">
    <li><a href="#UC-1:_Synchronizing_video_stream_and_HMI_.28e.g..2C_remote_surgery.29"><span class="tocnumber">2.2.1</span> <span class="toctext">UC-1: Synchronizing video stream and HMI (e.g., remote surgery)</span></a></li>
    <li><a href="#UC-3:_Wearable_devices"><span class="tocnumber">2.2.2</span> <span class="toctext">UC-3: Wearable devices</span></a></li>
    <li><a href="#UC-4:_MMI_output_devices:_Controlling_3D_Printers_using_a_remote_HMI"><span class="tocnumber">2.2.3</span> <span class="toctext">UC-4: MMI output devices: Controlling 3D Printers using a remote HMI</span></a></li>
    <li><a href="#UC-5:_MMI_input_devices:_Video_Cameras_work_with_HMI_and_sensors"><span class="tocnumber">2.2.4</span> <span class="toctext">UC-5: MMI input devices: Video Cameras work with HMI and sensors</span></a></li>
    <li><a href="#UC-6:_Multimodal_Guide_Device_at_Museums"><span class="tocnumber">2.2.5</span> <span class="toctext">UC-6: Multimodal Guide Device at Museums</span></a></li>
    <li><a href="#UC-8:_MIDI-based_Speech_Synthesizer"><span class="tocnumber">2.2.6</span> <span class="toctext">UC-8: MIDI-based Speech Synthesizer</span></a></li>
    <li><a href="#UC-9:_Collaboration_of_multiple_video_cameras"><span class="tocnumber">2.2.7</span> <span class="toctext">UC-9: Collaboration of multiple video cameras</span></a></li>
    <li><a href="#UC-10:_Geolocation_device.2C_e.g..2C_GPS.2C_as_a_MC_for_Location-based_Services"><span class="tocnumber">2.2.8</span> <span class="toctext">UC-10: Geolocation device, e.g., GPS, as a MC for Location-based Services</span></a></li>
    <li><a href="#UC-11:_Smart_Power_Meter"><span class="tocnumber">2.2.9</span> <span class="toctext">UC-11: Smart Power Meter</span></a></li>
    <li><a href="#UC-13:_MMI_Automatic_visual_data_annotator"><span class="tocnumber">2.2.10</span> <span class="toctext">UC-13: MMI Automatic visual data annotator</span></a></li>
    <li><a href="#UC-14:_Multimodal_e-Text"><span class="tocnumber">2.2.11</span> <span class="toctext">UC-14: Multimodal e-Text</span></a></li>
    <li><a href="#UC-15:_English_standardized_tests_through_an_MMI_interface._OPENPAU_project"><span class="tocnumber">2.2.12</span> <span class="toctext">UC-15: English standardized tests through an MMI interface. OPENPAU project</span></a></li>
    <li><a href="#UC-16:_Remote_watching_using_video_camera_and_MMI_interfaces"><span class="tocnumber">2.2.13</span> <span class="toctext">UC-16: Remote watching using video camera and MMI interfaces</span></a></li>
    <li><a href="#UC-17:_User_interface_as_a_sensor"><span class="tocnumber">2.2.14</span> <span class="toctext">UC-17: User interface as a sensor</span></a></li>
    <li><a href="#UC-20_Emergency_evacuation_information_and_notification_system"><span class="tocnumber">2.2.15</span> <span class="toctext">UC-20 Emergency evacuation information and notification system</span></a></li>
    <li><a href="#UC-21_Collaborative_session_by_remote_players"><span class="tocnumber">2.2.16</span> <span class="toctext">UC-21 Collaborative session by remote players</span></a></li>
    </ol>
  </li>
  </ol>
</li>
</ol>
</nav>


<h2><span class="mw-headline" id="Template_for_Use_Cases">1 Template for Use Cases</span></h2>
<p>This is a generic template for proposing use cases.
</p><p><b>Submitter(s):</b> <i>company</i> (<i>name</i>), ...
</p><p><b>Tracker Issue ID</b>: <a rel="nofollow" class="external text" href="http://www.w3.org/2011/webtv/track/issues/NN">ISSUE-NN</a>
</p><p><b>Category:</b>
How would you categorize this issue?
</p><p><b>Class:</b>
<i>Various MCs</i>
</p>
<ol>
<li> gap in existing specifications (=&gt; IG to draft a proposal for an existing WG), or how does this work in existing architecture?
</li>
<li> what implications does this UC have for discovery and registration?
</li>
<li> require new specification/WG (=&gt; IG to draft a proposal for W3C Director)
</li>
<li> can be addressed as part of a guidelines document to be produced by the IG (=&gt; The proponent should draft some text for the document)
</li>
</ol>
<p><b>Status:</b>
Who would work on this use case, progress, etc.
</p><p><b>Relationship between Entities included in this use case:</b>
</p>
<img src="images/entities.svg" alt="relationship-between-entities" />

<p><b>Actions:</b>
</p>
<ul>
<li> change the content depending on the visitors
</li>
<li> visitor-&gt;museum exhibit
<ul>
<li> Functions
<ul>
<li> Identify: visitors (children/matured, accessibility, languages, etc.)  
</li>
<li> Track: visitor near-by (location; patterns of behavior; situation of the visitor, e.g., whether in a dangerous area or not)
</li>
<li> Accountability: counting visitors
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><br />
<b>Description:</b> 
</p>
<ul>
<li> <i>high level description/overview of the goals of the use case</i>
</li>
<li> <i>Schematic Illustration (devices involved, work flows, etc)</i> (Optional)
</li>
<li> <i>Implementation Examples (e.g. JS code snippets)</i> (Optional)
</li>
</ul>
<p><b>Motivation:</b> 
</p>
<ul>
<li> <i>Explanation of benefit to ecosystem</i>
</li>
<li> <i>Why were you not able to use only existing standards to accomplish this?</i>
</li>
</ul>
<p><b>Dependencies:</b>
</p>
<ul>
<li> <i>other use cases, proposals or other ongoing standardization activities which this use case is dependent on or related to</i>
</li>
</ul>
<p><b>Viewpoint:</b>
</p>
<ul>
<li> <i>for example, specific industry (health care, hospitality industry, retail, automotive), accessibility or daily life, from both the user's viewpoint and the provider's viewpoint.</i> 
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> <i>What are the new requirements, i.e. functions that don't exist, generated by this use case?</i>
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> <i>any relevant comment that is good to track related to this use case</i>
</li>
</ul>

<h2><span class="mw-headline" id="Use_Cases">2 Use Cases</span></h2>
<p><b>Classes of Use Cases:</b>
</p>
<ol>
<li> Platform-level Use Cases, e.g., synchronizing multiple MCs
</li>
<li> Chunk of MC Use Cases, e.g., MMI output and MMI input
</li>
<li> Various MCs as extensios of Web Applications
</li>
</ol>
<p><b>Note on Accessibility/Security/Privacy/Safety</b>
</p>
<ul>
<li> We need to think about horizontal issues, e.g., accessibility, security, privacy and safety, for each use case to make the MMI-based systems even more useful.
</li>
<li> User authentication, device authentication, physical danger and safety are typical viewpoints.
</li>
<li> Authentication using SCXML should be fine but can become complex if you want to hide part of a screen from someone who's watching, for example.
</li>
<li> Safety would be especially important for people with disabilities,
</li>
<li> We need to think about the relationship between accessibility and security, if everything is connected.
</li>
<li> Making the system easier to access for people with disabilities should not make it easy for evil people to access.
</li>
<li> We'd like to update the UC template for that purpose.
</li>
</ul>

<h3><span class="mw-headline" id="High_priority_use_cases">2.1 High priority use cases</span></h3>

<!-- ### -->
<h4><span class="mw-headline" id="UC-2:_Interaction_with_Robots">2.1.1 UC-2: Interaction with Robots</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Kaz</i> &amp; <i>Debbie</i>
</p><p><b>Reviewer(s):</b> Masahiro, Kosuke, Shinya
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> @@@ 
</p><p><b>Class:</b>
<i>Various MCs</i>
</p><p><b>Status:</b>
The Japanese participants would be the best people to elaborate on this. Helena and Raj may also be interested.
</p><p><b>Description:</b> 
</p>
<ul>
<li> controlling robots (including both industrial robots and personal robots) using multimodal interface like voice and gesture
</li>
<li> A typical example is interaction of multiple pet robots, e.g., <a rel="nofollow" class="external text" href="https://en.wikipedia.org/wiki/Pepper_%28robot%29">Pepper</a>, <a rel="nofollow" class="external text" href="https://en.wikipedia.org/wiki/AIBO">AIBO</a>, Nao (French), Robi (Japanese) and human(s). Interoperable robot interaction?
</li>
</ul>
<ul>
<li> may use BehaviorML or VRML for the MC
</li>
<li> Behavior Tree could be used for IM
</li>
<li> possible requirements identified during TPAC breakout session:
<ul>
<li> offline operation
</li>
<li> scheduled operation
</li>
<li> realtime operation
</li>
<li> authorization
</li>
<li> discovery and vocabularies
</li>
<li> human to machine
</li>
<li> machine to machine
</li>
<li> privacy
</li>
<li> virtual robot/agent (software) to robot
</li>
<li> asynchronous operation
</li>
<li> messaging standards
</li>
<li> graceful degradation
</li>
<li> should be possible to control other devices through a robot instead of through smart phoneor remote
</li>
</ul>
</li>
</ul>
<p><br />
<b>Motivation:</b> 
</p>
<ul>
<li> Interactive and adaptive control is useful in the case of accidents or errors
</li>
<li> Elder people will increase in the near future and people would need help (or would enjoy interaction) with pet robots.
</li>
</ul>
<p><b>Dependencies:</b>
</p>
<ul>
<li> List of possible related standardization activities...
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li> What can't be done with the existing mechanism?
</li>
</ul>
<p>The current MMI Life Cycle events don't have the concept of scheduling or events scheduled to occur in the future. The simplest approach is to use the ExtensionNotification event, but that's not very standardized. Other ideas would be to use SMIL (but that's mostly for media), MIDI, SCXML, use the EMMA output functionality, or introduce a new attribute into the Life Cycle events. In addition to starting at a certain time, scheduling also introduces the idea of recurring events, for example, every day at 5:00.
</p>
<ul>
<li><b>Options</b>
<ol>
<li>Data field of ExtensionNotification: This could be used, but the idea of scheduling is very generic and should be standardized.
</li>
<li>SMIL: SMIL is used for presenting synchronized media, it doesn't seem appropriate for scheduling Life Cycle events, which may not be user-facing.
</li>
<li>EMMA: The reasons against using the EMMA output functionality are similar to the arguments against using SMIL. EMMA output is designed for output that will be consumed by a user. 
</li>
<li>MIDI: MIDI is also designed for output to a user.
</li>
<li>SCXML: the "delay" and "delayexpr" attributes of "&lt;send&gt;" may be suitable for scheduling. 
</li>
<li>Add a new common field or fields to the Life Cycle events. 
</li>
<li>MC's could also have an internal scheduling capability, independent of the IM
</li>
<li>EMMA 2.0 output could also be responsible for scheduling, using "emma:planned-output", perhaps using SMIL, should look at starting relative to other events
</li>
<li>could also use EMMA 2.0 for scheduling interactions that don't directly affect UI (scheduling DVR, for example)
</li>
</ol>
</li>
<li>The choice between 5 and 6 depends at least in part on whether the IM or the MC is responsible for scheduled action. If the IM is responsible for scheduling, SCXML "delay" would be more appropriate, because it would keep track of the schedule and send the Life Cycle events at the correct times. On the other hand, if the MC is responsible, it can be sent, for example, a StartRequest with a "delay", and the MC will be responsible for starting at the correct time. It may be that both are needed. If we want to maintain the independence of the MMI Architecture from SCXML, we can just say the IM (however it's implemented) is responsible for scheduling.
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> New features? APIs? data model? language?
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> Anything you want to add
</li>
</ul>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-7:_Multimodal_Appliances">2.3.4 UC-7: Multimodal Appliances</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Kaz, Dirk (Harman)</i>
</p><p><b>Reviewer(s):</b> Shinya, Masahiro, Kosuke, Ryuichi
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> @@@
</p><p><b>Class:</b>
<i>Various MCs</i>
</p><p><b>Status:</b>
The Japanese participants would be the best people to elaborate on this
</p><p><b>Description:</b> 
</p>
<ul>
<li> control home appliances like rice cooker using multimodal interface like speech and gesture
</li>
<li> user, location and orientation are identified, and appropriate service is provided
</li>
<li> e.g., digital TV with speech interface
</li>
<li> HMI which helps people use appliances at home easily, e.g., assisting people's sight by enlarging the image, assisting their hearing by clear audio and haptic interface
</li>
<li> wearable devices and AR-ready display are nice to be included. maybe a smart car also could be a target.
</li>
<li> see also <a rel="nofollow" class="external text" href="http://scxmlworkshop.de/eics2015/submissions/Multimodal%20Dialog%20Management%20in%20a%20Smart%20Home%20Context%20with%20SCXML.pdf">Dirk's paper at the SCXML worksohp</a>
</li>
<li> we should consider what is the (significant) merit to use multimodal interface for multiple-device integration
  <ul>
  <li> for example, we could integrate multiple devices and multiple Web services so that we can make an order to buy a pizza</li>
  <li> this is related to the UC-2, interaction with robots. robots could be used as smart/friendly/easy-to-use remotes for appliances</li>
  <li> we can talk to a pet robot and ask him/her to turn on TV, change the temp. of air conditioner, or get a can of juice from fridge.</li>
  <li> it is a kind of "personal agent" which remember each person's preference</li>
  </ul>
</li>
<li>Smart remote for appliances (former UC-18 which is merged with UC-7 here)
  <ul>
  <li> We could talk with a cute pet robot and we could ask him/her to control specific appliances or bring us a snack. The robot could interact with the world or with devices. </li>
  <li> The robot could be an intermediary between the user and devices.</li>
  <li> The robot could also be considered a kind of "user agent" that can do things for the user.</li>
  </ul>
</li>
</ul>
<p><b>Motivation:</b> 
</p>
<ul>
<li> @@@
</li>
</ul>
<p><b>Dependencies:</b>
</p>
<ul>
<li> List of possible related standardization activities...
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li> What can't be done with the existing mechanism?
</li>
<li> In In the paper cited above, the authors found that it was hard for the developers to be familiar with all the standards
</li>
<li> In this project it was hard to discover devices and all their capabilities
</li>
<li> The authors also discovered that they needed shortcuts for feedback. If the user's command has to be combined with information from the environment to understand the command. The information would just be sent to the fusion engine, instead of to the IM.
</li>
<li> There should also be a shared data model that could be used by the fusion and fission engines as well as the IM (but not the MC's)
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> New features? APIs? data model? language?
</li>
<li> A shared data model, like a blackboard, that could be used by the fusion and fission components and the upper interaction manager
</li>
<li> Separate fission and fusion components
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> Anything you want to add
</li>
</ul>
<p><b>Note:</b>"UC-18 smart remote for appliances" has been merged with this use case.</p>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-12:_Smart_Car_Platform">2.1.3 UC-12: Smart Car Platform</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Kaz, Dirk (Harman)</i>
</p><p><b>Reviewer(s):</b>
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> 1. gap in existing specifications
</p><p><b>Class:</b>
<i>Various MCs</i>
</p><p><b>Status:</b> 
Dirk would be the best person to elaborate on this. 
</p><p><b>Description:</b> 
</p>
<ul>
<li> Car navigation system, HTML5-based IVI, smartphones and sensor devices within car controlled using MMI lifecycle events and (JSON version of) EMMA over WebSocket, use cases are navigation, entertainment, and telephony
</li>
<li> There is an open source approach called GENIVI (Generic In Vehicle Infotainment). They have developed a vocabulary component that could be considered as the Data Component in the MMI Architecture. For the GUI part of the interaction the GENIVI Alliance use Qt or HTML5.
</li>
<li> MMI lifecycle events could be used to integrate multiple devices and modalities within a car (and outside the car)
</li>
</ul>
<p><b>Motivation:</b> 
</p>
<ul>
<li>Most current car architectures are based on Genivi, employing Qt for the GUI part of the interaction and FRANCA (commonapi) for communication issues. The current architecture should be extended to enable interaction with components designed after the W3C MMI architecture. This aims for an easy extension of existing deployments with support of additional modalities, devices and sensors within and outside the car.</li>
<li>An interaction manager will take care about the business logic and accessing the existing components like HMI, Vehicle Interface, Phone, ...</li>
<li>FRANCA may be used to deliver MMI lifecycle events.
</li>
</ul>
<p><b>Dependencies:</b>
</p>
<ul>
<li> Possible related standardization activities
<ul>
<li> The <a href="https://www.w3.org/auto/wg/">Automotive Working Group</a> is developing an approach to low-level interaction in the vehicle with Web Sockets.
</li>
<li>The <a href="http://www.genivi.org/">Genivi Alliance</a> is aiming for a resuable IVI platform as a flexible but standard reference architecture that may serve as a blueprint for building a full IVI solution
</li>
</ul>
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li>Existing standards like Genivi are not prepared to be enhanced by additional modalities. However, the combination of Qt and Franca for automotive apps is kind of similar to HTML5 and SCXML for Web apps</li>
<li>GENIVI guys have started to consider the combination of HTML5 and JS for IVI apps</li>
<li>A consistent multimodal dialog concept is missing to allow for a seamless user experience across modalities.</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li>Mapping of existing components to Modality Components and Interaction Managers as well as messaging concepts</li>
<li>Introduction of a topmost Interaction Manager to allow for a consistent user experience across modalities</li>
<li>Addition of new, so far unknown, modality components into the overall concept</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li>This effort should work closely with the related standardization activies to increase acceptance of suggested solutions.
</li>
</ul>
</div>


<!-- ### -->
<h4><span class="mw-headline" id="UC-19_Handling_millions_of_components">2.1.4 UC-19 Handling millions of components</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Kaz, Debbie, Helena</i>
</p><p><b>Reviewer(s):</b> <i>Masairo</i>, <i>Shinya</i>
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> @@@ 
</p><p><b>Class:</b>
<i>Platform level</i>
</p><p><b>Status:</b> 
Helena would be the best person to elaborate on this.
</p><p><b>Description:</b> 
</p>
<ul>
<li> everyone could have a personal user agent modality component
</li>
<li> the IM could handle millions of devices
</li>
<li> typical example is having a whole remote orchestra
</li>
<li> car rental agency needs to handle many users and many cars
</li>
<li> there could be an IM for the car rental company as well as an IM for each car
</li>
<li> the car rental agency IM could talk with the user agent component for each user
</li>
<li> the user agent could tell the car rental agency what the user' preferences are (this user is very tall, etc.)
</li>
<li> a university could interact with the user agents for each student and staff
</li>
<li> the car could interact with the parking lot manager about available spaces
</li>
<li> lots of privacy and security issues
</li>
<li> there might be a lot of clients that don't have a global vision, then there's a higher level manager
</li>
</ul>
<p><br />
<b>Motivation:</b> 
</p><p><b>Dependencies:</b>
</p>
<ul>
<li> List of possible related standardization activities...
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li> What can't be done with the existing mechanism?
</li>
<li> Are there any problems with scaling?
</li>
</ul>
<p><b>Requirements</b>
</p>
<ul>
<li> need a specific time keeper (time code generator) like the conductor of an orchestra
</li>
<li> need more than levels of accuracy for time management depending on each use case
</li>
<li> if there are slow MCs, need to delay other MCs to synchronize all the MCs
</li>
<li> The IM must be able to handle millions of devices, each one has an IM itself, we could use a complex MC for this, but we don't have any examples. The devices might be in families of clusters. We need a way to cluster, which could be with complex components, but we need an example. 
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> New features? APIs? data model? language?
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> Anything you want to add
</li>
</ul>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-19_Handling_millions_of_components">2.1.4 UC-19 Handling millions of components</span></h4>
<div class="usecase">
<p><b>Note:</b> There two "UC-19" sections. May I merge these???</p>
<p><b>Submitter(s):</b> <i>Kaz, Debbie, Helena</i>
<p><b>Category:</b> 2 does have implications for discovery and registration?</p>
<p><b>Class:</b> <i>Various MCs</i>
<p><b>Status:</b> Submitted to be commented.</p>
<p><b>Objects</b> </p>

<table class="wikitable" border="1">
<caption>Fuctions of Objects</caption>
<tr>
<th> Object </th>
<th> Identify </th>
<th> Track </th>
<th> Accountability
</th></tr>
<tr>
<th><span class="mw-headline">User Devices</span></th>
<td>Multiple user devices.</td>
<td> @
</td>
<td> @
</td></tr>
<tr>
  <th>Master Display </th>
  <td>The display to execute the current application.</td>
  <td>&nbsp;</td>
  <td>&nbsp;</td>
</tr>
<tr>
  <th>Master Haptics Actuator </th>
  <td>The haptics actuator used in the current appication.</td>
  <td>&nbsp;</td>
  <td>&nbsp;</td>
</tr>
<tr>
  <th>Master Audio </th>
  <td>The component rendering the audio content.</td>
  <td>&nbsp;</td>
  <td>&nbsp;</td>
</tr></table>
<p><b>Actions</b>
</p>
<ul>
<li>System prepares: search for devices in the network.</li>
<li>Each device is registered.</li>
<li>Interaction occurs (i.e. application loads).</li>
<li>Depending on the devices available, the application displays the visual contents on a Master Display choosed by the system.</li>
 <li>Depending on the devices available, the application renders the auditive contents on a Master Audio choosed by the system.</li>
 <li>Depending on the devices available, the application executes haptic contents on a Master Haptics Actuator choosed by the system.</li>
</ul>
<p><br />
<b>Description:</b> 
</p>
<ul>
<li></li>
</ul>
<p><b>Motivation:</b> 
</p>
<ul>
<li> </li>
</ul>
<p><b>Dependencies:</b>
</p>
<ul>
<li></li></ul>
<p><b>Viewpoint:</b>
</p>
<ul>
<li><i>Home Automation, Entertainment, Medical environments.</i> 
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> <i>Registration and Discovery. The behavior of the Ressources Manager indexing processes to store the device adresses and capabilities.</i></li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li>No comments for the moment.
</li>
</ul>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-22_Smart_Hotel_Room">2.1.5 UC-22 Smart Hotel Room</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Debbie Dahl</i>
</p><p><b>Tracker Issue ID</b>: 
</p><p><b>Category:</b>
</p><p><b>Class:</b>
<i>Various MCs</i>
How would you categorize this issue?
</p>
<ol>
<li> gap in existing specifications (=&gt; IG to draft a proposal for an existing WG), or how does this work in existing architecture?
</li>
</ol>
<p>See <a rel="nofollow" class="external free" href="https://www.w3.org/2002/mmi/2016/hotelUseCase/">https://www.w3.org/2002/mmi/2016/hotelUseCase/</a> for an example of how this would work in the existing architecture.
</p>
<ol>
<li> what implications does this UC have for discovery and registration?
</li>
</ol>
<p>Discovery and Registration functions are very important for this use case:
</p>
<ul>
<li>Users may be controlling the room by means of a mobile device, which must be able to find and make use of the room's services
</li>
<li>Although hotel room functions are very similar to each other (lighting, HVAC, entertainment, for example) there are often differences (new services) so that an application needs to be able to adapt itself to whatever capabilities the room offers.
</li>
</ul>
<ol>
<li> require new specification/WG: This can be addressed through existing specifications (EMMA, MMI Architecture, possibly ARIA), additional work on early stage specifications in progress (Discovery and Registration), existing work on security and privacy, and possibly new work on biometrics (for user authorization).
</li>
<li> can be addressed as part of a guidelines document to be produced by the IG (=&gt; The proponent should draft some text for the document)
</li>
</ol>
<p>The requirements can be partially addressed through the existing MMI Architecture specification. 
</p><p><br />
<b>Actions</b>
</p>
<ul>
<li> Control a hotel room with a natural UI that takes into account user preferences* hotel guest-&gt;hotel room
<ul>
<li> Functions
<ul>
<li> Identify: guest (authorized guest, etc.) Make sure that the user is authorized to enter and control the room
</li>
<li> Control: lighting, HVAC, entertainment, surveillance camera, locks, notify housekeeping, any other room functions
</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><b>Description:</b> 
</p>
<ul>
<li> <i>high level description/overview of the goals of the use case</i>
</li>
</ul>
<p>This use case describes a Smart Hotel Room, which authorizes an arriving guest to control the room and access its capabilities. Because the guest will only be staying in the room a short time and doesn't have time to learn a special GUI interface, a natural (speech or language) user interface is important. In addition, because rooms vary in their capabilities, a discovery process is essential, so that the user can become aware of the room's capabilities. Security and privacy are important for keeping the guest safe. Also, levels of authorization will be needed. For example, a child guest might be allowed to control the lights, but not the HVAC. Finally, note that this use case is closely related to UC-20 on emergency notifications. An application that allows a user to control a hotel room could also offer emergency notifications. 
</p>
<ul>
<li> <i>Schematic Illustration (devices involved, work flows, etc)</i> (Optional)
</li>
<li> see <a rel="nofollow" class="external free" href="https://www.w3.org/2002/mmi/2016/hotelUseCase/">https://www.w3.org/2002/mmi/2016/hotelUseCase/</a> for details.
</li>
</ul>
<p><b>Motivation:</b> 
</p>
<ul>
<li> <i>Explanation of benefit to ecosystem</i>
</li>
</ul>
<p>Reduces user frustration when users enter a new environment, especially when there are accessibility issues. for example, guests with limited mobility might find it difficult to use some physical controls. 
</p>
<ul>
<li> <i>Why were you not able to use only existing standards to accomplish this?</i>
</li>
</ul>
<p>There needs to be additional work on discovery and registration and on security and privacy to make this use case possible. 
</p><p><b>Dependencies:</b>
</p>
<ul>
<li> <i>other use cases, proposals or other ongoing standardization activities which this use case is dependent on or related to</i>
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> <i>What are the new requirements, i.e. functions that don't exist, generated by this use case?</i>
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> <i>any relevant comment that is good to track related to this use case</i>
</li>
</ul>


<p><b>Submitter(s):</b> <i>company</i> (<i>name</i>), ...
</p>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-23_Collaborative_content_sharing_in_a_smart_space">2.1.6 UC-23 Collaborative content sharing in a smart space</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Branimir (Wacom)</i>
<p><b>Category:</b> 2 
does have implications for discovery and registration?</p>
</p><p><b>Class:</b>
<i>Various MCs</i>
<p><b>Status:</b>
  Who would work on this use case, progress, etc.
</p><p><b>Objects</b>
</p>

<table class="wikitable" border="1">
<caption>Fuctions of Objects</caption>
<tr>
<th> Object </th>
<th> Identify </th>
<th> Track </th>
<th> Accountability
</th></tr>
<tr>
<th><span class="mw-headline">User Device</span></th>
<td>One or multiple user devices.</td>
<td> @
</td>
<td> @
</td></tr>
<tr>
<th> Master Device
</th>
<td> The  device of the moderator.</td>
<td> @
</td>
<td> @
</td></tr>
<tr>
  <th>Master Display </th>
  <td>The display to share visual content.</td>
  <td>&nbsp;</td>
  <td>&nbsp;</td>
</tr>
<tr>
  <th>Master Audio </th>
  <td>To share audio content.</td>
  <td>&nbsp;</td>
  <td>&nbsp;</td>
</tr></table>
<p><b>Actions</b>
</p>
<ul>
<li>System prepares: loads its screen, loads audio facilities, prepares monitoring of devices, defines lilits of range -room boundaries-.</li>
<li> Master device registers.</li>
<li>Each participant registers.</li>
<li>Interaction occurs. 4 kinds of interactions are possible: user initiated, moderator initiated, and interruptions from user, moderator or the system.</li>
<li>A participant leave or all the participants leave the room.</li>
<li>The system goes to stand-by mode.</li>
</ul>
<p><br />
<b>Description:</b> 
</p>
<ul>
<li> <i>Students in a classroom each have a mobile device where they can take inked or typed notes and they like to share their work with the classroom in a shared screen.</i>
</li>
<li><i>The content can be created and edited in realtime.</i></li>
<li><i>The use case must </i>be for authoritarian models and also the collaborative ones. A moderator can be needed in the case of a classroom o large meeting configurations.</li>
</ul>
<p><b>Motivation:</b> 
</p>
<ul>
<li> <i>Collaborative workspaces should be enhanced. Frictionless collaborative workspaces. </i> Saving time in conferences, classrooms.
</li>
<li><i>Web standards are not allowing a way to seamesly register or unregister devices. Ink is not a full fledged modality in collaboration workins cases. Ink is a slower way a communication rather than typing. The creative expression is constricted by inputs like typing. We dont have yet web standards for discovery and registration.</i>
</li>
</ul>
<p><b>Dependencies:</b>
</p>
<ul>
<li><i>The second screen work in the Web&amp;TV Interest Group,</i> Web signage Bussines Group, Web of Things Interest group, Echonet, DLNA (UPnP).</li>
</ul>
<p><b>Viewpoint:</b>
</p>
<ul>
<li><i>Education, Home Automation, Medical, Bussines environments.</i> 
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> <i>Registration and Discovery. The behavior of the Ressources Manager (using SCXML) to monitor the device presence or absence.</i></li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> <i>any relevant comment that is good to track related to this use case</i>
</li>
</ul>
</div>


<h3><span class="mw-headline" id="Other_use_cases">2.2 Other use cases</span></h3>

<!-- ### -->
<h4><span class="mw-headline" id="UC-1:_Synchronizing_video_stream_and_HMI_.28e.g..2C_remote_surgery.29">2.1.1 UC-1: Synchronizing video stream and HMI (e.g., remote surgery)</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>W3C</i> (<i>Kaz Ashimura</i>)
</p><p><b>Reviewer(s):</b> Masahiro, Kosuke, Shinya
</p><p><b>Tracker Issue ID:</b> <a rel="nofollow" class="external text" href="http://www.w3.org/2011/webtv/track/issues/NN">ISSUE-NN</a>
</p><p><b>Category:</b>
<i>1. gap with existing specification</i>
</p><p><b>Class:</b>
<i>Platform level</i>
</p><p><b>Description:</b> 
</p>
<ul>
<li> <i>The ability to synchronize (1) a video stream using holography and (2) advanced HMI like robot arm for remote surgery.</i>
  <ul>
  <li>That would requires precise synchronization for realtime interaction. Also it would be nicer to have 3-d display and HMI to control the image from any angles using intuitive operation regardless of the user's knowledge on the system.</li>
  </ul>
</li>
<li> Typical example is the smart HMI from a famous movie of "Minority Report".
</li>
<li> It is expected to handle various devices provided by multiple vendors.
</li>
</ul>
<p><b>Motivation:</b> 
</p>
<ul>
<li> <i>This is being requested by the TV industry.</i>
</li>
<li> <i>It may be possible to achieve this with existing web standards but this should be investigated.</i>
</li>
</ul>
<p><br /> 
<b>Dependencies:</b>
</p>
<ul>
<li> There are other activities going on in the W3C related to this use case. For example, Multi-Device Community Group, Web and TV Interest Group, Second Screen Working Group and possibly the Web of Things IG as well. It may be the case that these other efforts will be able to support the required functionality, but if there needs to be any MMI work, we should coordinate with the other groups. Also there is a proposal that there could be an HTML5 new features community group, and we should coordinate with that.
</li>
</ul>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-3:_Wearable_devices">2.3.2 UC-3: Wearable devices</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Kaz</i>
</p><p><b>Reviewer(s):</b> Shinya, Yasuhiro
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> @@@ 
</p><p><b>Class:</b>
<i>Various MCs</i>
</p><p><b>Description:</b>
</p>
<ul>
<li> Abstract: 
<ul>
<li> controlling wearable devices (like sphygmomanometer or game devices) using multimodal interface like voice and gesture
<ul>
<li> may get connected with home network when the user comes home
</li>
<li> may interact with head mount display like Google Glass
</li>
<li> doctors might want to use
</li>
</ul>
</li>
<li> controlling devices, e.g., music instruments, by eye tracking or brain wave
</li>
<li> e.g.) <a rel="nofollow" class="external text" href="http://japangiving.jp/c/11343">Eye Play the Piano</a>
</li>
</ul>
</li>
</ul>
<ul>
<li>Problem:
<ul>
<li>There are various wearable devices, but currently the usage of wearable devices depends on identification and personalization using vendor specific account information, e.g., Google account or Apple ID.  It also depends on smartphone's connectivity outside home.
</li>
</ul>
</li>
</ul>
<ul>
<li> Possible technology areas to attach:
<ul>
<li> There is a question on how to identify and manage the capability of each wearable device and what kind interaction could be made between users and devices.
</li>
<li> MMI could be used to identify and manage devices and interactions.
</li>
<li> For that, what are the key requirements to solve the issues:
<ul>
<li> make devices on different platforms interact with each other?
</li>
<li> a smarter mechanism to identify multiple devices and let them interact with each other without each developer's specifying device capability explicitly
</li>
</ul>
</li>
</ul>
</li>
</ul>
<ul>
<li> Related slides (Japanese): <a rel="nofollow" class="external free" href="http://www.slideshare.net/mawarimichi/ss-37559346">http://www.slideshare.net/mawarimichi/ss-37559346</a>
</li>
</ul>
<p><br />
<b>Motivation:</b> 
</p>
<ul>
<li> MMI integrates various user interface modalities (and devices which provide modlities) for WoT purposes. And wearable technology incorporates computer and advanced UI technolgies with clothing and accessories. So wearable technology is a good use case for MMI.
</li>
</ul>
<p><b>Dependencies:</b>
</p>
<ul>
<li> List of possible related standardization activities...
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li> What can't be done with the existing mechanism?
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> New features? APIs? data model? language?
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> Anything you want to add
</li>
</ul>
<p><br />
</p>
</div>


<!-- ### -->
<h4><span class="mw-headline" id="UC-4:_MMI_output_devices:_Controlling_3D_Printers_using_a_remote_HMI">2.2.1 UC-4: MMI output devices: Controlling 3D Printers using a remote HMI</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Kaz</i>
</p><p><b>Reviewer(s):</b> @@@
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> @@@ 
</p><p><b>Class:</b>
<i>Chunk of MC</i>
</p><p><b>Description:</b> 
</p>
<ul>
<li> interaction with a 3D printer or vending machine using multimodal interface like voice and gesture
</li>
<li> controlling a 3D printer using multiple modalities including gesture and speech.
</li>
<li> may be applied to automatic cooker
</li>
<li> this is an output device for MMI
</li>
</ul>
<p><b>Motivation:</b> 
</p>
<ul>
<li> @@@
</li>
</ul>
<p><b>Dependencies:</b>
</p>
<ul>
<li> List of possible related standardization activities...
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li> What can't be done with the existing mechanism?
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> New features? APIs? data model? language?
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> Anything you want to add
</li>
</ul>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-5:_MMI_input_devices:_Video_Cameras_work_with_HMI_and_sensors">2.2.2 UC-5: MMI input devices: Video Cameras work with HMI and sensors</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Kaz</i>
</p><p><b>Reviewer(s):</b> @@@
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> @@@ 
</p><p><b>Class:</b>
<i>Chunk of MC</i>
</p><p><b>Description:</b> 
</p>
<ul>
<li> interaction with a video camera, e.g., back view monitor for cars, using multimodal interface like voice and gesture
</li>
<li> a video camera outside the entrance detects intruders and let us via an HMI
</li>
<li> a video camera could be installed on a drone and controlled using gesture, speech, etc. 
</li>
</ul>
<p><b>Motivation:</b> 
</p>
<ul>
<li> @@@
</li>
</ul>
<p><b>Dependencies:</b>
</p>
<ul>
<li> List of possible related standardization activities...
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li> What can't be done with the existing mechanism?
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> New features? APIs? data model? language?
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> Anything you want to add
</li>
</ul>
<p><br />
</p>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-6:_Multimodal_Guide_Device_at_Museums">2.2.3 UC-6: Multimodal Guide Device at Museums</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Kaz</i>
</p><p><b>Reviewer(s):</b> Shinya, Masahiro, Kosuke
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> @@@ 
</p><p><b>Class:</b>
<i>Various MCs</i>
</p><p><b>Description:</b> 
</p>
<ul>
<li> not only audio but also any kind of modalities can be used, e.g., input from the user using gesture, speech, shaking the device
</li>
<li> interaction between the user and the device is the key
</li>
<li> location and orientation are identified, and appropriate service is provided
</li>
</ul>
<p><b>Motivation:</b> 
</p>
<ul>
<li> @@@
</li>
</ul>
<p><b>Dependencies:</b>
</p>
<ul>
<li> List of possible related standardization activities...
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li> What can't be done with the existing mechanism?
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> New features? APIs? data model? language?
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> This UC could be applied to other places/scenarios than museums.
</li>
</ul>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-8:_MIDI-based_Speech_Synthesizer">2.3.5 UC-8: MIDI-based Speech Synthesizer</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Kaz</i>
</p><p><b>Reviewer(s):</b> Masahiro, Shinya, Kosuke
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> @@@ 
</p><p><b>Class:</b>
<i>Various MCs</i>
</p><p><b>Description:</b> 
</p>
<ul>
<li> control MIDI-based voice generator with a prosody generator to make it a speech synthesizer  using multimodal interface
</li>
<li> not only speech synthesizers but also music synthesizers, music play devices, sirens at factories could be included
</li>
<li> the following is a lit of example input modalities: gesture, movement of body parts (lips, eyes, eye blow, winking), humming, wearable suit
</li>
<li> this use case reminds us of possible "semantic interpretation for MMI" which allows us to specify the semantics of the input
</li>
<li> an existing example is Yamaha's <a rel="nofollow" class="external text" href="https://en.wikipedia.org/wiki/Miburi">Miburi system</a> (<a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=TThN0kMqFys">demo video</a>)
</li>
</ul>
<p><b>Motivation:</b> 
</p>
<ul>
<li> @@@
</li>
</ul>
<p><b>Dependencies:</b>
</p>
<ul>
<li> List of possible related standardization activities...
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li> What can't be done with the existing mechanism?
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> New features? APIs? data model? language?
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> Anything you want to add
</li>
</ul>
<p><br />
</p>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-9:_Collaboration_of_multiple_video_cameras">2.1.2 UC-9: Collaboration of multiple video cameras</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Kaz</i>
</p><p><b>Reviewer(s):</b> @@@
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> @@@ 
</p><p><b>Class:</b>
<i>Platform level</i>
</p><p><b>Description:</b> 
</p>
<ul>
<li> radio-controlled cars and helicopters collaboratively work with each other
</li>
<li> each of them has video camera
</li>
<li> need to identify their position and direciton
</li>
</ul>
<p><b>Motivation:</b> 
</p>
<ul>
<li> @@@
</li>
</ul>
<p><b>Dependencies:</b>
</p>
<ul>
<li> List of possible related standardization activities...
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li> What can't be done with the existing mechanism?
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> New features? APIs? data model? language?
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> would be better to be merged with UC-5
</li>
</ul>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-10:_Geolocation_device.2C_e.g..2C_GPS.2C_as_a_MC_for_Location-based_Services">2.3.6 UC-10: Geolocation device, e.g., GPS, as a MC for Location-based Services</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Kaz</i>
</p><p><b>Reviewer(s):</b> Kosuke
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> @@@ 
</p><p><b>Class:</b>
<i>Various MCs</i>
</p><p><b>Description:</b> 
</p>
<ul>
<li> radio-controlled cars and helicopters collaboratively work with each other
</li>
<li> each of them has video camera
</li>
<li> need to identify their position and direciton
</li>
</ul>
<p><b>Motivation:</b> 
</p>
<ul>
<li> @@@
</li>
</ul>
<p><b>Dependencies:</b>
</p>
<ul>
<li> List of possible related standardization activities...
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li> What can't be done with the existing mechanism?
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> New features? APIs? data model? language?
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> Anything you want to add
</li>
</ul>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-11:_Smart_Power_Meter">2.3.7 UC-11: Smart Power Meter</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Kaz</i>
</p><p><b>Reviewer(s):</b>
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> @@@ 
</p><p><b>Class:</b>
<i>Various MCs</i>
</p><p><b>Description:</b> 
</p>
<ul>
<li> smart power meter
</li>
</ul>
<p><b>Motivation:</b> 
</p>
<ul>
<li> @@@
</li>
</ul>
<p><b>Dependencies:</b>
</p>
<ul>
<li> List of possible related standardization activities...
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li> What can't be done with the existing mechanism?
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> New features? APIs? data model? language?
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> Anything you want to add
</li>
</ul>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-13:_MMI_Automatic_visual_data_annotator">2.3.9 UC-13: MMI Automatic visual data annotator</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Helena</i>
</p><p><b>Reviewer(s):</b>
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> @@@ 
</p><p><b>Class:</b>
<i>Various MCs</i>
</p><p><b>Description:</b> 
</p>
<ul>
<li> @@@
</li>
</ul>
<p><b>Motivation:</b> 
</p>
<ul>
<li> In image recognition systems, the coordination between a semantic web inference engine and the low-level attributes recognition services can be synchronized by using MMI lifecycle events.
</li>
<li> MMI lifecycle events could be used to integrate complementary and concurrent services within a recognition process
</li>
</ul>
<p><b>Dependencies:</b>
</p>
<ul>
<li> List of possible related standardization activities...
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li> Currently it is very complex to produce well synchronized semantic fusion for multimodal recognition in the fashion media.
</li>
<li> The recognition process is very specialized on vision techniques and it lacks of the inference strengths provided by a high level vision coming from semantic web ontologies
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> New features? APIs? data model? language?
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> Anything you want to add
</li>
</ul>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-14:_Multimodal_e-Text">2.3.10 UC-14: Multimodal e-Text</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Kaz</i>
</p><p><b>Reviewer(s):</b>
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> @@@ 
</p><p><b>Class:</b>
<i>Various MCs</i>
</p><p><b>Description:</b>
</p>
<ul>
<li> Multiple modalities to be used to access the contents for e-Learning
</li>
<li> MMI Architecture could be used to synchronize those multiple modalities. 
</li>
</ul>
<p><b>Motivation:</b> 
</p>
<ul>
<li> Text books for e-Learning mainly use text and graphics using, e.g., HTML, CSS, SVG and MathML.
</li>
<li> However, from the viewpoint of accessibility and better understanding for learners it would be better to have multimodal interface, e.g., speech recognition/synthesis, handwriting/gesture recognition, to access the e-Learning contents.
</li>
</ul>
<p><b>Dependencies:</b>
</p>
<ul>
<li> HTML5, CSS
</li>
<li> SMIL, SSML, SRGS/SISR, PLS
</li>
<li> Second Screen, Multi-device timing
</li>
<li> Annotation
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li> TBD
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> New features? APIs? data model? language?
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> (nothing so far)
</li>
</ul>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-15:_English_standardized_tests_through_an_MMI_interface._OPENPAU_project">2.3.11 UC-15: English standardized tests through an MMI interface. OPENPAU project</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Jesus Garcia-Laborda, Teresa  Magal-Royo</i>
</p><p><b>Reviewer(s):</b>
</p><p><b>Tracker Issue ID:</b> 
</p><p><b>Category:</b> <i>Computer Aided Learning language, Accesibility, MMI educational interfaces, Second language skill acquisition.</i>
</p><p><b>Class:</b>
<i>Various MCs</i>
</p><p><b>Description:</b> 
The OPENPAU is a research project funded by the Spanish Government for 3 years and has worked on a navigation interface (keyboard, voice and touchscreen) to develop english tests using Moodle©.
</p><p>1.    Multiple modalities have been used to access the contents for second language acquisition e-learning.
</p><p>2.    The MMI Architecture has been used to synchronize multiple modalities of input navigation.
</p><p><b>Motivation:</b>
</p><p>Specific developments to improve the accessibility of educational content for language learning environments by using MMI interfaces or via keyboard, voice and touchscreen.
</p><p><b>Dependencies:</b>
</p><p>1.	HTML5, CSS 
2.	Moodle©
3.	TabletPc
4.	Accesibility 
5.	Chrome (voice recognition browser automatically integrated)
</p><p><b>Gaps</b>
</p><p>1.       Specific developments in Moodle© (OS) that allow access to educational content accessible via keyboard, voice and touchscreen synchronously.
</p><p>2.       Specific examinations or tests of skill acquisition in languages as a second language in Moodle © (OS) that allows access to educational content accessible via keyboard, voice and touchscreen synchronously.
</p><p><b>What needs to be standardized:</b>
</p><p>6.       Specific educational developments in Moodle © (OS) that allows access to content accessible via keyboard, voice and touch in synchronously.
</p><p>7.       Oriented user interfaces in conducting tests that allow the use of integrated MMI navigation.
</p><p><b>Comments:</b>
</p>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-16:_Remote_watching_using_video_camera_and_MMI_interfaces">2.3.12 UC-16: Remote watching using video camera and MMI interfaces</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Kaz</i>
</p><p><b>Reviewer(s):</b> Masahiro, Kosuke, Shinya
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> @@@ 
</p><p><b>Class:</b>
<i>Various MCs</i>
</p><p><b>Description:</b> 
</p>
<ul>
<li> watching factories and homes remotely using sensors and video cameras
</li>
<li> the output could be notification on the Web browser on our mobiles or simply sirens/audio signals at the factory or the 
</li>
</ul>
<p>home
</p>
<ul>
<li> should be useful to support aged people, children, etc.
</li>
<li> can use both specific vendor-proprietary sensors and standard ones
</li>
<li> question on how to integrate more than one factories and homes at once
<ul>
<li> may be good to have an meta IM at the headquarters of the company which communicates with the sub IMs from all the factories of the company
</li>
</ul>
</li>
<li> this discussion makes me think about who in the home should be the IM, and how all the devices inside the home should be integrated
<ul>
<li> maybe we could have sub IMs for each room which handles devices within the assigned rooms
</li>
<li> that implies each IM needs to talk with other (via the main IM of the home) to check which device is located at which room and negotiate with each other to recognize some device like tablet TV is moved from the living to the bedroom
</li>
</ul>
</li>
</ul>
<p><b>Motivation:</b> 
</p><p><b>Dependencies:</b>
</p>
<ul>
<li> List of possible related standardization activities...
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li> What can't be done with the existing mechanism?
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> New features? APIs? data model? language?
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> Anything you want to add
</li>
</ul>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-17:_User_interface_as_a_sensor">2.1.2 UC-17: User interface as a sensor</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Helena</i>
</p><p><b>Reviewer(s):</b>
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> @@@ 
</p><p><b>Class:</b>
<i>Various MCs</i>
</p><p><b>Description:</b> 
</p>
<ul>
<li> Many smart pictures are on the web, and each one is a sensor that detects user interaction
</li>
<li> It's necessary to detect a click on the controller, the brand name, the , the scrolling on the page and the suggestion
</li>
<li> It's important for the link to be correct
</li>
<li> It's important to detect and record scrolling because it means that the user is reading, so the MC will store the event and the position
</li>
<li> There are also passive sensors. A smart picture MC can be created
</li>
</ul>
<p><b>Motivation:</b> 
</p><p><b>Dependencies:</b>
</p>
<ul>
<li> List of possible related standardization activities...
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li> What can't be done with the existing mechanism?
</li>
</ul>
<p><b>Requirements</b>
</p>
<ul>
<li> be able to update data on the modality component
</li>
<li> be able to update data on the IM
</li>
<li> needs a "suspended state"
</li>
<li> need of a virtualization process to complete the sensoring process
</li>
<li> use of similarity data
</li>
<li> bidirectional communication between Resources Manager and Modality Components
</li>
<li> interface changes managed by the system (Resources Manager) and not user input (the MC)
</li>
</ul>
<p><b>What needs to be standardized:</b>
</p>
<ul>
<li> New features? APIs? data model? language?
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> Anything you want to add
</li>
</ul>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-20_Emergency_evacuation_information_and_notification_system">2.3.15 UC-20 Emergency evacuation information and notification system</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Kaz, Debbie</i>
</p><p><b>Reviewer(s):</b>
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> @@@ 
</p><p><b>Class:</b>
<i>Various MCs</i>
</p><p><b>Description:</b> 
</p>
<ul>
<li> people might have different preference to get information on fire evacuation
</li>
<li> what if a big apartment or hotel, or a small house with two floors
</li>
<li> possibly multiple places on fire within a town
</li>
</ul>
<ul>
<li> barrier-free safety
</li>
</ul>
<ul>
<li> would apply to many kinds of buildings, house, apartment, office building, hotelor other public spaces 
</li>
</ul>
<ul>
<li> or even more general than that, you want it to be aware of where the fire is, for example and route people away from that
</li>
</ul>
<ul>
<li> could be more general than fire
</li>
</ul>
<ul>
<li> there are many relevant sensors from the WoT group, e.g., camera, heat, water or something for earthquakes; smoke detectors, all reporting their state to the IM which would notify the users
</li>
</ul>
<ul>
<li> a friendly robot could help with this
<ul>
<li> it could help you physically or tell you things
</li>
<li> could help children or other people who can't use a smartphone
</li>
<li> could talk with the home server and ask the server to open the door
</li>
<li> the robot could talk with the parents first
</li>
</ul>
</li>
</ul>
<p><b>Motivation:</b> 
</p><p><b>Dependencies:</b>
</p>
<ul>
<li> List of possible related standardization activities...
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li> What can't be done with the existing mechanism?
</li>
<li> Are there any problems with scaling?
</li>
</ul>
<p><b>Requirements</b>
</p><p><b>What needs to be standardized:</b>
</p>
<ul>
<li> New features? APIs? data model? language?
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> Anything you want to add
</li>
</ul>
</div>

<!-- ### -->
<h4><span class="mw-headline" id="UC-21_Collaborative_session_by_remote_players">2.1.4 UC-21 Collaborative session by remote players</span></h4>
<div class="usecase">
<p><b>Submitter(s):</b> <i>Kaz, Masahiro, Shinya</i>
</p><p><b>Reviewer(s):</b>
</p><p><b>Tracker Issue ID:</b> @@@
</p><p><b>Category:</b> @@@ 
</p><p><b>Class:</b>
<i>Platform level</i>
</p><p><b>Description:</b> 
</p>
<ul>
<li> multiple players collaboratively play a musical session
<ul>
<li> piano player in US
</li>
<li> guitar player in France
</li>
<li> singer in Japan
</li>
</ul>
</li>
</ul>
<ul>
<li> a computer-based musical instrument could be included
<ul>
<li> e.g.) <a rel="nofollow" class="external text" href="https://www.youtube.com/watch?v=QoQ9slb3F5Q">Yamaha Disklavier</a>
</li>
</ul>
</li>
</ul>
<p><b>Motivation:</b> 
</p><p><b>Dependencies:</b>
</p>
<ul>
<li> List of possible related standardization activities...
</li>
</ul>
<p><b>Gaps</b>
</p>
<ul>
<li> What can't be done with the existing mechanism?
</li>
<li> Are there any problems with scaling?
</li>
</ul>
<p><b>Requirements</b>
</p><p><b>What needs to be standardized:</b>
</p>
<ul>
<li> New features? APIs? data model? language?
</li>
</ul>
<p><b>Comments:</b>
</p>
<ul>
<li> Anything you want to add
</li>
</ul>



</div>

</body>
</html>
